export type ExampleSlug = "transformer" | "resnet" | "bert";

export type ExampleSection = {
  id: string;
  title: string;
  content: string;
};

export type ExampleDownload = {
  label: string;
  href: string;
};

export type ExampleWalkthrough = {
  slug: ExampleSlug;
  title: string;
  paperTitle: string;
  summary: string;
  badges: string[];
  sections: ExampleSection[];
  downloads: ExampleDownload[];
};

export const EXAMPLE_WALKTHROUGHS: Record<ExampleSlug, ExampleWalkthrough> = {
  "transformer": {
    "slug": "transformer",
    "title": "Transformer",
    "paperTitle": "Attention Is All You Need",
    "summary": "Existing models for tasks like machine translation, which process sequences of words, faced two main challenges: they were either slow because they had to process words one after another, or they struggled to understand how distant words in a long sentence related to each other.",
    "badges": [
      "architecture",
      "code map",
      "training recipe"
    ],
    "sections": [
      {
        "id": "problem",
        "title": "1. What It Does",
        "content": "Existing models for tasks like machine translation, which process sequences of words, faced two main challenges: they were either slow because they had to process words one after another, or they struggled to understand how distant words in a long sentence related to each other. This paper introduces the Transformer, a novel model that completely abandons these traditional approaches. Instead, it relies entirely on a mechanism called 'attention,' which allows the model to simultaneously look at all other words in a sentence and decide which ones are most relevant to understanding the current word. The Transformer uses this attention mechanism in both an 'encoder' that processes the input sentence and a 'decoder' that generates the output sentence, ensuring it still understands the order of words by adding special 'positional encodings.' This design is revolutionary because it allows the model to process all words in a sentence simultaneously, dramatically speeding up training compared to older methods. Crucially, it also makes it much easier for the model to identify connections between words, no matter how far apart they are in a sentence. As a result, the Transformer achieved state-of-the-art performance on complex tasks like machine translation, setting a new standard for how sequence data is processed in AI."
      },
      {
        "id": "mechanism",
        "title": "2. The Mechanism",
        "content": "### Briefing Section 2: The Mechanism\n\nThe Transformer model processes sequence-to-sequence tasks using an encoder-decoder architecture, as illustrated in Figure 1. The encoder maps an input sequence into a continuous representation, which the decoder then uses to generate an output sequence one token at a time. The entire process is built upon attention mechanisms, eschewing recurrence and convolutions.\n\n#### 2.1. Input and Positional Encoding\n\nThe process begins by converting input tokens into high-dimensional vectors.\n\n1.  **Token Embedding**: Both the source and target sequences are first passed through separate embedding layers to convert each token ID into a vector of dimension `d_model` = 512.\n2.  **Positional Encoding**: Since the model contains no recurrent or convolutional layers, it has no inherent sense of token order. To provide this crucial information, positional encodings are added to the input embeddings. These encodings are fixed, non-learned vectors calculated using sine and cosine functions of different frequencies:\n\n    > PE<sub>(pos, 2i)</sub> = sin(pos / 10000<sup>2i / d_model</sup>)\n    > PE<sub>(pos, 2i+1)</sub> = cos(pos / 10000<sup>2i / d_model</sup>)\n\n    Here, `pos` is the position of the token in the sequence, and `i` is the dimension within the embedding vector. This sinusoidal approach allows the model to learn relative positional relationships, as the encoding for any position `pos + k` can be represented as a linear function of the encoding for position `pos` (Section 3.5).\n\n#### 2.2. The Encoder Stack\n\nThe encoder's role is to build a rich, context-aware representation of the entire input sequence. It consists of a stack of `N`=6 identical layers. The output of the final encoder layer is a sequence of vectors, one for each input token, which is then passed to every layer in the decoder. Each encoder layer has two sub-layers.\n\n##### 2.2.1. Multi-Head Self-Attention\n\nThe first sub-layer allows each token in the input sequence to look at all other tokens in the sequence to better inform its own representation. This is the core mechanism for understanding context. It is built from a fundamental unit called **Scaled Dot-Product Attention**, depicted in Figure 2 (left).\n\nThe attention score is calculated as:\n\n> Attention(Q, K, V) = softmax( (QK<sup>T</sup>) / √d<sub>k</sub> ) V --- (Eq. 1)\n\n-   **Prerequisite**: The input to this layer is a sequence of vectors. For each vector, three new vectors are created through linear projections: a **Query (Q)**, a **Key (K)**, and a **Value (V)**. In self-attention, Q, K, and V are all derived from the same input sequence (the output of the previous layer).\n-   **Step 1: Score Calculation**: The dot product of a query vector `Q` with all key vectors `K` is computed to produce a raw similarity score between them.\n-   **Step 2: Scaling**: This score is scaled down by dividing by √d<sub>k</sub>, where `d_k` is the dimension of the key vectors (here, `d_k`=64). This scaling is necessary because for large values of `d_k`, the dot products can become very large, pushing the softmax function into regions with vanishingly small gradients, which would impede learning (Section 3.2.1).\n-   **Step 3: Weighting**: A softmax function is applied to the scaled scores to obtain attention weights, which are positive and sum to one. These weights determine how much focus to place on each input token when encoding the current token.\n-   **Step 4: Output**: The final output for the query is a weighted sum of all the value vectors `V`, using the computed attention weights.\n\nInstead of performing a single attention function, the model employs **Multi-Head Attention** (Figure 2, right). This involves projecting the Q, K, and V vectors `h`=8 times with different, learned linear projections. Each of these projected versions of Q, K, and V is fed into a separate attention \"head.\" The outputs of these 8 heads are then concatenated and projected again with another learned linear transformation to produce the final output. This allows the model to jointly attend to information from different representation subspaces at different positions, enriching its ability to capture complex relationships (Section 3.2.2).\n\n##### 2.2.2. Position-wise Feed-Forward Network\n\nThe second sub-layer is a simple, fully connected feed-forward network (FFN) applied to each position's vector independently. It consists of two linear transformations with a ReLU activation in between:\n\n> FFN(x) = max(0, xW<sub>1</sub> + b<sub>1</sub>)W<sub>2</sub> + b<sub>2</sub> --- (Eq. 2)\n\n-   Here, `x` is the output from the attention sub-layer. `W_1` and `b_1` are the weight matrix and bias for the first linear transformation (which expands the dimension from `d_model`=512 to `d_ff`=2048), and `W_2` and `b_2` are for the second (which projects it back to `d_model`=512). This sub-layer provides non-linearity and further transforms the representations.\n\n##### 2.2.3. Residual Connections and Normalization\n\nEach of the two sub-layers (Multi-Head Attention and FFN) in a layer has a residual connection around it, followed by layer normalization (Section 3.1). The output of each sub-layer is `LayerNorm(x + Sublayer(x))`. This is a critical component that allows for the training of a deep stack of `N` layers by preventing gradients from vanishing and stabilizing the learning process.\n\n#### 2.3. The Decoder Stack\n\nThe decoder's role is to generate the output sequence one token at a time, using the encoder's output as context. It also consists of a stack of `N`=6 identical layers. For each step in the output sequence, the decoder takes the previously generated tokens as input.\n\n-   **Prerequisite**: The decoder input is the target sequence, \"shifted right.\" This means that for predicting the token at position `i`, the decoder is only given the ground-truth tokens from positions 1 to `i-1`. This preserves the **auto-regressive** property, ensuring predictions are based only on past information.\n\nEach decoder layer has three sub-layers.\n\n##### 2.3.1. Masked Multi-Head Self-Attention\n\nThis sub-layer is nearly identical to the self-attention mechanism in the encoder. However, it is modified to prevent positions from attending to subsequent positions. This is achieved by applying a \"look-ahead mask\" inside the Scaled Dot-Product Attention (the \"Mask (opt.)\" step in Figure 2). Before the softmax step, the mask sets all values corresponding to future positions to negative infinity, effectively zeroing out their attention weights. This is necessary to maintain the auto-regressive property during training.\n\n##### 2.3.2. Multi-Head Cross-Attention\n\nThis second sub-layer is what connects the encoder and decoder. It performs multi-head attention, but its inputs are different:\n-   The **Queries (Q)** come from the output of the previous decoder sub-layer (the masked self-attention).\n-   The **Keys (K)** and **Values (V)** come from the output of the final layer of the encoder stack.\n\nThis mechanism allows every position in the decoder to attend over all positions in the input sequence, enabling it to weigh the importance of different parts of the source sentence when generating the next target token.\n\n##### 2.3.3. Position-wise Feed-Forward Network\n\nThis third sub-layer is identical in structure and function to the FFN in the encoder layer.\n\nAs in the encoder, each of these three sub-layers is wrapped with a residual connection and layer normalization.\n\n#### 2.4. Final Output Generation\n\nAfter the final decoder layer produces its output vectors, a final linear transformation projects these vectors into a much larger vector with dimensions equal to the size of the target vocabulary. A softmax function is then applied to this vector to convert the scores (logits) into a probability distribution. The token with the highest probability is chosen as the output for that time step."
      },
      {
        "id": "prereqs",
        "title": "3. Prerequisites",
        "content": "Here are the foundational concepts required to understand the \"Attention Is All You Need\" paper, presented in a dependency-ordered list:\n\n### 1. Recurrent Neural Networks (RNNs)\n\n1)  **Problem**\n    How can a neural network process sequential data (like sentences) where the order of elements is crucial and the input length can vary? Traditional feed-forward networks struggle with variable-length inputs and maintaining context over time.\n\n2)  **Solution**\n    RNNs introduce a 'memory' or 'hidden state' that is updated at each step of the sequence. The output at a given step `t` is a function of the input at step `t` and the hidden state from step `t-1`. This recurrent loop allows information to persist through the sequence, capturing temporal dependencies. Variants like LSTMs and GRUs address vanishing/exploding gradients in long sequences.\n\n3)  **Usage in this paper**\n    The Transformer is explicitly designed to replace RNNs. The paper argues that the inherently sequential nature of RNNs (where `h_t` depends on `h_{t-1}`) is a major bottleneck for parallelization during training and limits their efficiency for very long sequences. The Transformer's attention-only architecture solves these limitations.\n\n### 2. Encoder-Decoder Architecture\n\n1)  **Problem**\n    How can a model handle sequence-to-sequence tasks (like machine translation or text summarization) where the input and output sequences can have different lengths, vocabularies, and grammatical structures?\n\n2)  **Solution**\n    This architecture is split into two main parts: an 'encoder' that reads the entire input sequence and transforms it into a rich, fixed-size context representation (or a sequence of context vectors), and a 'decoder' that takes this context and generates the output sequence one element at a time, conditioned on previously generated elements.\n\n3)  **Usage in this paper**\n    The Transformer follows this high-level encoder-decoder structure. The left side of Figure 1 in the paper illustrates the encoder stack, which processes the input sentence, and the right side shows the decoder stack, which generates the translated sentence.\n\n### 3. Attention Mechanisms\n\n1)  **Problem**\n    In the basic encoder-decoder architecture (especially with RNNs), the entire meaning of a long input sequence is often compressed into a single, fixed-size context vector by the encoder. This creates an information bottleneck, making it difficult for the decoder to access specific, relevant details from the input when generating later parts of the output sequence.\n\n2)  **Solution**\n    Attention allows the decoder, at each step of generating an output, to dynamically look back at all parts of the encoder's output (or its own previous outputs). It computes a set of 'attention weights' to determine which input parts are most relevant for the current output step and creates a weighted average of these parts as a dynamic context vector. This provides a flexible way to access information without a fixed-size bottleneck.\n\n3)  **Usage in this paper**\n    Attention is the core building block of the Transformer, replacing recurrence and convolutions entirely. It is used in three distinct ways:\n    *   **Self-attention in the encoder:** Input tokens attend to other input tokens to build richer representations.\n    *   **Masked self-attention in the decoder:** Output tokens attend to previous output tokens to maintain the auto-regressive property during generation.\n    *   **Cross-attention between encoder and decoder:** The decoder attends to the encoder's output, mimicking traditional attention mechanisms to align input and output.\n\n### 4. Residual Connections (Skip Connections)\n\n1)  **Problem**\n    As neural networks get deeper (i.e., have many layers), they become very difficult to train effectively. A common issue is the 'vanishing gradient' problem, where gradients shrink exponentially as they are backpropagated through many layers, preventing weights in early layers from updating and learning.\n\n2)  **Solution**\n    Residual (or 'skip') connections add the input of a layer (or a block of layers) directly to its output. Mathematically, if `F(x)` is the output of a layer, the residual connection makes the output `x + F(x)`. This creates a direct path for the gradient to flow through the network, mitigating the vanishing gradient problem and allowing for the training of much deeper models without significant performance degradation.\n\n3)  **Usage in this paper**\n    Residual connections are employed extensively throughout the Transformer. They are used around each of the two sub-layers (the multi-head attention sub-layer and the position-wise feed-forward network sub-layer) in every encoder and decoder layer. The paper specifies the operation as `LayerNorm(x + Sublayer(x))`.\n\n### 5. Layer Normalization\n\n1)  **Problem**\n    During training, the distribution of each layer's inputs changes as the parameters of the previous layers change. This phenomenon, called 'internal covariate shift', can slow down training, make it unstable, and require careful initialization and lower learning rates.\n\n2)  **Solution**\n    Layer Normalization stabilizes the distributions by normalizing the inputs to a layer. For each training example and for each layer, it computes the mean and variance across all features (or hidden units) for that *single example* and uses them to rescale the inputs. This helps to speed up and stabilize training, making deep networks less sensitive to initialization and more robust.\n\n3)  **Usage in this paper**\n    Layer Normalization is applied after each residual connection in both the encoder and decoder layers. It is a critical component for stabilizing the training of the deep Transformer architecture, especially given its post-normalization placement (`LayerNorm(x + Sublayer(x))`).\n\n### 6. Label Smoothing\n\n1)  **Problem**\n    When training a classification model with a softmax output and one-hot labels (e.g., `[0, 1, 0]`), the model is encouraged to make its predictions extremely confident (pushing one logit to a very high positive value and others to very low negative values). This can lead to over-fitting, poor calibration (overestimating probabilities), and reduced generalization ability, especially if the training data contains noise or mislabeled examples.\n\n2)  **Solution**\n    Label smoothing replaces the hard 0 and 1 targets with soft targets. For example, a target of `[0, 1, 0]` might be changed to `[0.05, 0.9, 0.05]` (where `0.05` is `epsilon_ls / (num_classes - 1)` and `0.9` is `1 - epsilon_ls`). This discourages the model from becoming overconfident, forces it to learn a more robust internal representation, and improves generalization.\n\n3)  **Usage in this paper**\n    Label smoothing with a value of `epsilon_ls = 0.1` is used as a regularization technique during training of the Transformer. The paper notes that while this technique might slightly increase the perplexity (a measure of how well the model predicts a sample), it consistently improves accuracy and the BLEU score (a common metric for machine translation quality)."
      },
      {
        "id": "implementation",
        "title": "4. Implementation Map",
        "content": "### Implementation-Oriented Walkthrough\n\nThe generated implementation map below is rendered exactly as code, preserving assumptions and provenance markers.\n\n```python\n# Component: Add & Norm\n# Provenance: paper-stated\n# Assumption: The `eps` parameter for `nn.LayerNorm` is assumed to be `1e-6`, which is a common default for numerical stability in PyTorch's `LayerNorm` implementation.\n# Assumption: The `d_model` parameter, representing the dimensionality of the model (and thus the feature dimension for LayerNorm), is inferred as a necessary input for `nn.LayerNorm` to specify the `normalized_shape` in the Transformer architecture.\nimport torch\nimport torch.nn as nn\n\nclass AddNorm(nn.Module):\n    \"\"\"\n    Implements the Add & Norm component as described in the paper.\n    This corresponds to the post-norm architecture: LayerNorm(x + Sublayer(x)).\n    \"\"\"\n    def __init__(self, d_model: int, eps: float = 1e-6):\n        super().__init__()\n        # INFERRED: d_model is the dimensionality of the model, required for LayerNorm.\n        #           It represents the feature dimension of the input to LayerNorm.\n        # ASSUMED: eps is a small value added to the variance for numerical stability in LayerNorm.\n        #          A common default is 1e-6 or 1e-5 in PyTorch's LayerNorm.\n        self.norm = nn.LayerNorm(d_model, eps=eps) # Eq. (N) - Layer Normalization\n\n    def forward(self, x: torch.Tensor, sublayer_output: torch.Tensor) -> torch.Tensor:\n        # Eq. (N) - Residual connection: x + Sublayer(x)\n        # The paper describes \"Add & Norm\" as applying LayerNorm after the residual connection.\n        # The ambiguity resolution 'layernorm_placement' confirms this exact order: `LayerNorm(x + Sublayer(x))`\n        return self.norm(x + sublayer_output)\n\n# Component: Decoder Layer\n# Provenance: paper-stated\n# Assumption: d_k = d_v = d_model / num_heads, as is standard for Transformer attention.\n# Assumption: Bias terms are initialized to zero, which is a common practice.\n# Assumption: Dropout is applied to the attention weights before multiplying with V inside MultiHeadAttention.\n# Assumption: Mask values are 0 for masked positions, 1 for unmasked positions.\n# Assumption: d_model must be divisible by num_heads for d_k to be an integer.\n# Assumption: bias=True for all linear layers based on ambiguity resolution 'bias_in_linear_layers'.\n# Assumption: Using nn.init.xavier_uniform_ as it's a standard choice for Xavier initialization. The ambiguity resolution 'weight_initialization' mentions \"variance-scaling initializer similar to Xavier uniform, scaling by (d_in + d_out) / 2\". nn.init.xavier_uniform_ scales by sqrt(6 / (fan_in + fan_out)), which is a common form of Xavier.\n# Assumption: Using a large negative number (-1e9) to effectively zero out masked attention scores after softmax for numerical stability.\n# Assumption: Using nn.init.kaiming_uniform_ as it's a standard choice for Kaiming initialization. The ambiguity resolution 'weight_initialization' states \"Kaiming (He) initialization\" for FFN. For the second layer of FFN, while it doesn't have a ReLU *after* it, it's common practice to use Kaiming for both layers in the FFN block if the first layer uses ReLU.\n# Assumption: Dropout is applied to the final output of the FFN sub-layer, as per ambiguity resolution 'dropout_placement_ffn'.\n# Assumption: LayerNorm placement is 'post-norm' as per ambiguity resolution 'layernorm_placement': LayerNorm(x + Sublayer(x)).\n# Assumption: Dropout for the residual connection is applied after the sub-layer output, before adding to the input. This is distinct from internal dropouts within MultiHeadAttention or FFN.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# Helper module: MultiHeadAttention\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n        super().__init__()\n        # ASSUMED: d_k = d_v = d_model / num_heads, as is standard for Transformer attention.\n        # INFERRED: d_model must be divisible by num_heads for d_k to be an integer.\n        if d_model % num_heads != 0:\n            raise ValueError(f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\")\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        # Linear projections for Q, K, V\n        # INFERRED: bias=True for all linear layers based on ambiguity resolution 'bias_in_linear_layers'.\n        self.q_linear = nn.Linear(d_model, d_model, bias=True)\n        self.k_linear = nn.Linear(d_model, d_model, bias=True)\n        self.v_linear = nn.Linear(d_model, d_model, bias=True)\n\n        # Output linear projection\n        # INFERRED: bias=True for all linear layers based on ambiguity resolution 'bias_in_linear_layers'.\n        self.out_linear = nn.Linear(d_model, d_model, bias=True)\n\n        # ASSUMED: Dropout is applied to the attention weights before multiplying with V.\n        self.dropout = nn.Dropout(dropout_rate)\n\n        self._reset_parameters() # For weight initialization\n\n    def _reset_parameters(self):\n        # Weight initialization: Xavier (Glorot) for attention projections.\n        # INFERRED: Using nn.init.xavier_uniform_ as it's a standard choice for Xavier initialization.\n        # The ambiguity resolution 'weight_initialization' mentions \"variance-scaling initializer similar to Xavier uniform, scaling by (d_in + d_out) / 2\".\n        # nn.init.xavier_uniform_ scales by sqrt(6 / (fan_in + fan_out)), which is a common form of Xavier.\n        nn.init.xavier_uniform_(self.q_linear.weight)\n        nn.init.xavier_uniform_(self.k_linear.weight)\n        nn.init.xavier_uniform_(self.v_linear.weight)\n        nn.init.xavier_uniform_(self.out_linear.weight)\n\n        # ASSUMED: Bias terms are initialized to zero, which is a common practice.\n        if self.q_linear.bias is not None:\n            nn.init.constant_(self.q_linear.bias, 0.)\n        if self.k_linear.bias is not None:\n            nn.init.constant_(self.k_linear.bias, 0.)\n        if self.v_linear.bias is not None:\n            nn.init.constant_(self.v_linear.bias, 0.)\n        if self.out_linear.bias is not None:\n            nn.init.constant_(self.out_linear.bias, 0.)\n\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None):\n        batch_size = query.size(0)\n\n        # 1) Linear projections and split into heads\n        # Eq. (1) (implicitly, as part of h_i = Attention(QW_Q, KW_K, VW_V))\n        q = self.q_linear(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        k = self.k_linear(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        v = self.v_linear(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n        # 2) Scaled Dot-Product Attention\n        # Eq. (1)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        if mask is not None:\n            # Apply mask (for self-attention, it's look-ahead mask; for cross-attention, it's padding mask)\n            # ASSUMED: Mask values are 0 for masked positions, 1 for unmasked positions.\n            # INFERRED: Using a large negative number (-1e9) to effectively zero out masked attention scores after softmax.\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1) # Eq. (1)\n        attn_weights = self.dropout(attn_weights) # ASSUMED: Dropout applied to attention weights before multiplying with V\n\n        context = torch.matmul(attn_weights, v) # Eq. (1)\n\n        # 3) Concatenate heads and apply final linear layer\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        output = self.out_linear(context) # Eq. (1)\n\n        return output, attn_weights\n\n# Helper module: PositionwiseFeedForward\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n        super().__init__()\n        # INFERRED: bias=True for all linear layers based on ambiguity resolution 'bias_in_linear_layers'.\n        self.w_1 = nn.Linear(d_model, d_ff, bias=True)\n        self.w_2 = nn.Linear(d_ff, d_model, bias=True)\n        # INFERRED: Dropout is applied to the final output of the FFN sub-layer, as per ambiguity resolution 'dropout_placement_ffn'.\n        self.dropout = nn.Dropout(dropout_rate)\n\n        self._reset_parameters() # For weight initialization\n\n    def _reset_parameters(self):\n        # Weight initialization: Kaiming (He) for FFN with ReLU activations.\n        # INFERRED: Using nn.init.kaiming_uniform_ as it's a standard choice for Kaiming initialization.\n        # The ambiguity resolution 'weight_initialization' states \"Kaiming (He) initialization\" for FFN.\n        nn.init.kaiming_uniform_(self.w_1.weight, nonlinearity='relu')\n        # For the second layer, while it doesn't have a ReLU *after* it, it's common practice\n        # to use Kaiming for both layers in the FFN block if the first layer uses ReLU.\n        nn.init.kaiming_uniform_(self.w_2.weight, nonlinearity='relu')\n\n        # ASSUMED: Bias terms are initialized to zero, which is a common practice.\n        if self.w_1.bias is not None:\n            nn.init.constant_(self.w_1.bias, 0.)\n        if self.w_2.bias is not None:\n            nn.init.constant_(self.w_2.bias, 0.)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Eq. (3)\n        # FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n        # Dropout applied to the output of FFN before residual connection, as per ambiguity resolution 'dropout_placement_ffn'.\n        return self.dropout(self.w_2(F.relu(self.w_1(x))))\n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout_rate: float, eps: float):\n        super().__init__()\n\n        # Masked Multi-Head Self-Attention sub-layer\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        # INFERRED: LayerNorm placement is 'post-norm' as per ambiguity resolution 'layernorm_placement': LayerNorm(x + Sublayer(x)).\n        self.self_attn_norm = nn.LayerNorm(d_model, eps=eps)\n        # INFERRED: Dropout for the residual connection is applied after the sub-layer output, before adding to the input.\n        self.self_attn_residual_dropout = nn.Dropout(dropout_rate)\n\n        # Multi-Head Encoder-Decoder Attention sub-layer\n        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        # INFERRED: LayerNorm placement is 'post-norm' as per ambiguity resolution 'layernorm_placement'.\n        self.cross_attn_norm = nn.LayerNorm(d_model, eps=eps)\n        # INFERRED: Dropout for the residual connection.\n        self.cross_attn_residual_dropout = nn.Dropout(dropout_rate)\n\n        # Feed-Forward Network sub-layer\n        # The FFN module itself includes the dropout as per ambiguity resolution 'dropout_placement_ffn'.\n        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n        # INFERRED: LayerNorm placement is 'post-norm' as per ambiguity resolution 'layernorm_placement'.\n        self.ffn_norm = nn.LayerNorm(d_model, eps=eps)\n        # No separate residual dropout here, as the FFN module already applies it internally as per 'dropout_placement_ffn'.\n\n    def forward(self,\n                x: torch.Tensor,\n                encoder_output: torch.Tensor,\n                src_mask: torch.Tensor,\n                tgt_mask: torch.Tensor) -> torch.Tensor:\n\n        # Masked Multi-Head Self-Attention sub-layer\n        # Post-norm architecture: LayerNorm(x + Sublayer(x))\n        # First, apply LayerNorm to the input of the sub-layer.\n        _x = self.self_attn_norm(x)\n        # Then, compute the sub-layer output.\n        self_attn_output, _ = self.self_attn(_x, _x, _x, tgt_mask) # Q, K, V, mask\n        # Apply dropout to the sub-layer output, then add to the original input (residual connection).\n        x = x + self.self_attn_residual_dropout(self_attn_output) # Eq. (2)\n\n        # Multi-Head Encoder-Decoder Attention sub-layer\n        # Post-norm architecture: LayerNorm(x + Sublayer(x))\n        _x = self.cross_attn_norm(x)\n        cross_attn_output, _ = self.cross_attn(_x, encoder_output, encoder_output, src_mask) # Q, K, V, mask\n        x = x + self.cross_attn_residual_dropout(cross_attn_output) # Eq. (2)\n\n        # Feed-Forward Network sub-layer\n        # Post-norm architecture: LayerNorm(x + Sublayer(x))\n        _x = self.ffn_norm(x)\n        # The FFN module already applies dropout to its output as per 'dropout_placement_ffn'.\n        ffn_output = self.ffn(_x)\n        x = x + ffn_output # Eq. (2)\n\n        return x\n\n# Component: Decoder Stack\n# Provenance: paper-stated\n# Assumption: d_model: TODO: Specify model dimension (e.g., 512).\n# Assumption: num_heads: TODO: Specify number of attention heads (e.g., 8).\n# Assumption: d_ff: TODO: Specify feed-forward inner dimension (e.g., 2048).\n# Assumption: dropout_rate: TODO: Specify dropout rate (e.g., 0.1).\n# Assumption: N: TODO: Specify number of decoder layers (e.g., 6).\n# Assumption: eps: TODO: Specify epsilon for LayerNorm (e.g., 1e-6).\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n# Helper: Scaled Dot-Product Attention\nclass ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Computes scaled dot-product attention.\n    \"\"\"\n    def __init__(self, dropout_rate):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n        # INFERRED: Dropout is applied to the attention weights before multiplying with V.\n        # This is standard practice in the original Transformer paper.\n\n    def forward(self, query, key, value, mask=None):\n        # Eq. (1)\n        d_k = query.size(-1)\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # Eq. (1)\n\n        if mask is not None:\n            # INFERRED: Use a large negative number for masked positions to ensure softmax outputs zero.\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        p_attn = F.softmax(scores, dim=-1) # Eq. (1)\n        p_attn = self.dropout(p_attn) # INFERRED: Dropout applied to attention probabilities.\n\n        return torch.matmul(p_attn, value), p_attn\n\n# Helper: Multi-Head Attention\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Implements Multi-Head Attention as described in the paper.\n    \"\"\"\n    def __init__(self, d_model, num_heads, dropout_rate):\n        super().__init__()\n        # ASSUMED: d_model, num_heads, dropout_rate are provided as hyperparameters.\n        self.d_model = d_model\n        self.num_heads = num_heads\n        # INFERRED: d_k (dimension per head) is d_model // num_heads.\n        # This must be an integer.\n        # Eq. (2)\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.d_k = d_model // num_heads\n\n        # Linear projections for Q, K, V, and output.\n        # Eq. (3)\n        # bias_in_linear_layers: Include bias terms for all linear transformations.\n        self.linears = nn.ModuleList([nn.Linear(d_model, d_model, bias=True) for _ in range(4)])\n        # INFERRED: The first three linears are for W_Q, W_K, W_V, and the last one is for W_O.\n\n        self.attention = ScaledDotProductAttention(dropout_rate)\n        # INFERRED: Output dropout for the MultiHeadAttention sub-layer is handled by SublayerConnection.\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # weight_initialization: Xavier (Glorot) initialization for attention projections.\n        # The tensor2tensor library used a variance-scaling initializer similar to Xavier uniform.\n        for i in range(4):\n            nn.init.xavier_uniform_(self.linears[i].weight)\n            if self.linears[i].bias is not None:\n                nn.init.constant_(self.linears[i].bias, 0.) # INFERRED: Bias initialized to zero.\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        # 1) Do all the linear projections in batch from d_model => num_heads x d_k\n        # Eq. (3)\n        query, key, value = [\n            l(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n            for l, x in zip(self.linears, (query, key, value))\n        ]\n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, self.attn = self.attention(query, key, value, mask=mask) # Eq. (1)\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) # Eq. (4)\n        return self.linears[-1](x) # Eq. (4)\n\n# Helper: Position-wise Feed-Forward Networks\nclass PositionwiseFeedForward(nn.Module):\n    \"\"\"\n    Implements the position-wise feed-forward network.\n    \"\"\"\n    def __init__(self, d_model, d_ff, dropout_rate):\n        super().__init__()\n        # ASSUMED: d_model, d_ff, dropout_rate are provided as hyperparameters.\n        # Eq. (5)\n        # bias_in_linear_layers: Include bias terms for all linear transformations.\n        self.w_1 = nn.Linear(d_model, d_ff, bias=True)\n        self.w_2 = nn.Linear(d_ff, d_model, bias=True)\n        # dropout_placement_ffn: Apply dropout only to the final output of the FFN sub-layer,\n        # i.e., `Dropout(FFN(x))`, before the residual connection. This means the dropout\n        # is handled by the SublayerConnection, not internally within this module.\n\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # weight_initialization: Kaiming (He) initialization for models with ReLU activations (like the FFN).\n        nn.init.kaiming_uniform_(self.w_1.weight, nonlinearity='relu')\n        if self.w_1.bias is not None:\n            nn.init.constant_(self.w_1.bias, 0.) # INFERRED: Bias initialized to zero.\n\n        # weight_initialization: For the second linear layer, Xavier (Glorot) is a standard choice.\n        nn.init.xavier_uniform_(self.w_2.weight)\n        if self.w_2.bias is not None:\n            nn.init.constant_(self.w_2.bias, 0.) # INFERRED: Bias initialized to zero.\n\n    def forward(self, x):\n        # Eq. (5)\n        # INFERRED: ReLU activation is used as per the paper.\n        return self.w_2(F.relu(self.w_1(x)))\n\n# Helper: SublayerConnection (Add & Norm)\nclass SublayerConnection(nn.Module):\n    \"\"\"\n    A residual connection followed by a layer normalization.\n    Implements the post-norm architecture: LayerNorm(x + Dropout(Sublayer(x))).\n    \"\"\"\n    def __init__(self, d_model, dropout_rate, eps=1e-6):\n        super().__init__()\n        # ASSUMED: d_model, dropout_rate, eps are provided as hyperparameters.\n        self.norm = nn.LayerNorm(d_model, eps=eps)\n        self.dropout = nn.Dropout(dropout_rate)\n        # layernorm_placement: Implement the post-norm architecture: LayerNorm(x + Sublayer(x)).\n        # This means LayerNorm is applied *after* the residual connection and dropout.\n\n    def forward(self, x, sublayer):\n        \"Apply residual connection to any sublayer with the same size.\"\n        # layernorm_placement: Post-norm architecture: LayerNorm(x + Sublayer(x))\n        # The dropout is applied to the sublayer output before adding to the residual.\n        return self.norm(x + self.dropout(sublayer(x)))\n\n# Decoder Layer\nclass DecoderLayer(nn.Module):\n    \"\"\"\n    One layer of the decoder.\n    Consists of masked multi-head self-attention, encoder-decoder attention, and a feed-forward network.\n    Each sub-layer is followed by a residual connection and layer normalization.\n    \"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout_rate, eps=1e-6):\n        super().__init__()\n        # ASSUMED: d_model, num_heads, d_ff, dropout_rate, eps are provided as hyperparameters.\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout_rate)\n        self.src_attn = MultiHeadAttention(d_model, num_heads, dropout_rate) # Encoder-Decoder Attention\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout_rate)\n        self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout_rate, eps) for _ in range(3)])\n        # INFERRED: Three sublayers in a decoder layer as per the paper's architecture.\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        \"Follows Figure 1 (right) for connections.\"\n        # Masked Multi-Head Self-Attention\n        # Eq. (1)\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n\n        # Multi-Head Encoder-Decoder Attention\n        # Eq. (1)\n        # INFERRED: Query comes from the decoder, Key/Value come from the encoder output (memory).\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n\n        # Position-wise Feed-Forward Network\n        # Eq. (5)\n        x = self.sublayer[2](x, self.feed_forward)\n        return x\n\n# Decoder Stack\nclass DecoderStack(nn.Module):\n    \"\"\"\n    A stack of N identical decoder layers.\n    \"\"\"\n    def __init__(self, N, d_model, num_heads, d_ff, dropout_rate, eps=1e-6):\n        super().__init__()\n        # ASSUMED: N (number of layers), d_model, num_heads, d_ff, dropout_rate, eps are provided as hyperparameters.\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout_rate, eps)\n            for _ in range(N)\n        ])\n        # INFERRED: A final LayerNorm is typically applied after the last decoder layer\n        # in the overall Transformer architecture before the final linear projection.\n        self.norm = nn.LayerNorm(d_model, eps=eps)\n\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)\n```"
      },
      {
        "id": "ambiguity",
        "title": "5. Missing Details",
        "content": "## weight_initialization: Weight Initialization Strategy\n- Type: missing_training_detail\n- Section: 3. Model Architecture\n- Ambiguous point: The paper does not specify how the weights of the various linear layers (in multi-head attention and feed-forward networks) and embedding layers are initialized.\n- Implementation consequence: Improper weight initialization can lead to training instability, such as exploding or vanishing gradients, or slow convergence. Different initialization schemes (e.g., Xavier/Glorot, Kaiming/He) can significantly impact the final model performance. Without this detail, reproducing the results is difficult.\n- Agent resolution: A common and effective strategy for models with ReLU activations (like the FFN) is Kaiming (He) initialization. For other layers, Xavier (Glorot) initialization is a standard choice. The tensor2tensor library, mentioned in the paper, used a variance-scaling initializer similar to Xavier uniform, scaling by `(d_in + d_out) / 2`.\n- Confidence: 0.5\n\n## layernorm_placement: Layer Normalization Placement (Pre-Norm vs. Post-Norm)\n- Type: underspecified_architecture\n- Section: 3.1 Encoder and Decoder Stacks\n- Ambiguous point: The paper states the output of each sub-layer is `LayerNorm(x + Sublayer(x))`. This is known as 'post-norm'.\n- Implementation consequence: Post-norm architectures, as described, can be difficult to train without a careful learning rate warmup, as the gradients can vanish or explode at the beginning of training for deep stacks. Later research has shown that 'pre-norm' (`x + Sublayer(LayerNorm(x))`) leads to more stable training and often removes the need for a slow learning rate warmup. Implementing post-norm exactly as described might make it harder to train the model from scratch.\n- Agent resolution: Implement the post-norm architecture as described, `LayerNorm(x + Sublayer(x))`, and ensure the learning rate schedule with warmup (`warmup_steps = 4000`) is also implemented exactly, as it is critical for stabilizing the training of this architecture.\n- Confidence: 0.5\n\n## bias_in_linear_layers: Use of Bias in Linear Layers\n- Type: underspecified_architecture\n- Section: 3.2.2 Multi-Head Attention & 3.3 Position-wise Feed-Forward Networks\n- Ambiguous point: The paper's formula for the Position-wise Feed-Forward Network, `max(0, xW1 + b1)W2 + b2`, explicitly includes bias terms (`b1`, `b2`). However, the description of Multi-Head Attention does not mention if the projection matrices `W_Q`, `W_K`, `W_V`, and `W_O` have corresponding bias terms.\n- Implementation consequence: If biases are incorrectly added or omitted from the attention projection layers, the parameter count of the model will be different, and the representational capacity of the attention heads could be affected. This could lead to a failure to replicate the reported performance.\n- Agent resolution: The standard implementation, and the one used in the official tensor2tensor library, is to include bias terms for all linear transformations, including the attention projections (`W_Q`, `W_K`, `W_V`, `W_O`) and the feed-forward layers.\n- Confidence: 0.5\n\n## positional_encoding_max_length: Maximum Sequence Length for Positional Encoding\n- Type: missing_hyperparameter\n- Section: 3.5 Positional Encoding\n- Ambiguous point: The paper describes a sinusoidal formula for positional encodings which can theoretically handle any sequence length. However, in practice, these are typically pre-computed into a fixed-size matrix for efficiency. The maximum length of this matrix is not specified.\n- Implementation consequence: If the pre-computed matrix is too small, the model will fail at inference time if given a sequence longer than the maximum length it was trained on. If it's unnecessarily large, it will consume excess memory. The choice of max length affects the model's ability to generalize to longer sequences, which the paper claims is a benefit of the sinusoidal method.\n- Agent resolution: Choose a maximum sequence length that is larger than any sequence encountered during training and typical for the task. A common choice is 512 or 1024 for machine translation, or 2048 for longer-form text tasks. The original implementation often used a default of 512 or set it based on the longest sequence in the training data.\n- Confidence: 0.5\n\n## dropout_placement_ffn: Dropout Placement within FFN\n- Type: underspecified_architecture\n- Section: 5.4 Regularization\n- Ambiguous point: The paper states dropout is applied 'to the output of each sub-layer'. For the FFN sub-layer, this means after the second linear transformation. It is not specified if dropout is also applied *inside* the FFN, for example, after the ReLU activation.\n- Implementation consequence: Adding an extra dropout layer inside the FFN would change the regularization scheme and could affect model performance and convergence. Many popular implementations (e.g., in PyTorch's `nn.Transformer`) do add dropout after the activation within the FFN, which would be a deviation from the paper's description.\n- Agent resolution: Follow the paper's description strictly: apply dropout only to the final output of the FFN sub-layer, i.e., `Dropout(FFN(x))`, before the residual connection. Do not add a separate dropout layer inside the FFN.\n- Confidence: 0.5\n\n## embedding_weight_sharing: Scope of Embedding Weight Sharing\n- Type: underspecified_architecture\n- Section: 3.4 Embeddings and Softmax\n- Ambiguous point: The paper states: 'we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation'. This could mean all three (input embedding, output embedding, pre-softmax linear) share one matrix, or that the output embedding and pre-softmax linear share one, and the input embedding is separate.\n- Implementation consequence: If all three matrices are shared, the model's parameter count is reduced, but it forces the input and output token representations into the same space, which might not be optimal. The more common practice is to only tie the weights of the output embedding and the pre-softmax linear layer, as they both map from the model's hidden dimension to vocabulary logits/embeddings.\n- Agent resolution: Implement weight sharing between the output embedding layer and the pre-softmax linear transformation. The input embedding layer should have its own separate weight matrix. This is the most common interpretation and implementation of this technique.\n- Confidence: 0.5"
      },
      {
        "id": "training",
        "title": "6. Training Recipe",
        "content": "### Hyperparameter Registry\n\n| name | value | source | status | suggested_default |\n| --- | --- | --- | --- | --- |\n| N (Encoder/Decoder Layers) | 6 | 3.1 Encoder and Decoder Stacks | paper-stated |  |\n| d_model (Model Dimension) | 512 | 3.1 Encoder and Decoder Stacks | paper-stated |  |\n| h (Number of Attention Heads) | 8 | 3.2.2 Multi-Head Attention | paper-stated |  |\n| d_k (Key Dimension) | 64 | 3.2.2 Multi-Head Attention | paper-stated |  |\n| d_v (Value Dimension) | 64 | 3.2.2 Multi-Head Attention | paper-stated |  |\n| d_ff (Feed-Forward Inner Dimension) | 2048 | 3.3 Position-wise Feed-Forward Networks | paper-stated |  |\n| P_drop (Dropout Rate, base model) | 0.1 | 5.4 Regularization | paper-stated |  |\n| epsilon_ls (Label Smoothing) | 0.1 | 5.4 Regularization | paper-stated |  |\n| Optimizer | Adam | 5.3 Optimizer | paper-stated |  |\n| Adam beta1 | 0.9 | 5.3 Optimizer | paper-stated |  |\n| Adam beta2 | 0.98 | 5.3 Optimizer | paper-stated |  |\n| Adam epsilon | 1e-9 | 5.3 Optimizer | paper-stated |  |\n| warmup_steps (Learning Rate Schedule) | 4000 | 5.3 Optimizer | paper-stated |  |\n| EN-DE Vocabulary Size | 37000 | 5.1 Training Data and Batching | paper-stated |  |\n| EN-FR Vocabulary Size | 32000 | 5.1 Training Data and Batching | paper-stated |  |\n| Batch Size (Tokens per batch) | 25000 source tokens and 25000 target tokens | 5.1 Training Data and Batching | paper-stated |  |\n| Training Steps (base model) | 100000 | 5.2 Hardware and Schedule | paper-stated |  |\n| Training Steps (big model) | 300000 | 5.2 Hardware and Schedule | paper-stated |  |\n| Hardware | 8 NVIDIA P100 GPUs | 5.2 Hardware and Schedule | paper-stated |  |\n| Beam Size (Inference) | 4 | 6.1 Machine Translation | paper-stated |  |\n| Length Penalty alpha (Inference) | 0.6 | 6.1 Machine Translation | paper-stated |  |\n| Positional Encoding Wavelength Max | 10000 | 3.5 Positional Encoding | paper-stated |  |\n| N (big model) | 6 | Table 3: Variations on the Transformer architecture | paper-stated |  |\n| d_model (big model) | 1024 | Table 3: Variations on the Transformer architecture | paper-stated |  |\n| d_ff (big model) | 4096 | Table 3: Variations on the Transformer architecture | paper-stated |  |\n| h (big model) | 16 | Table 3: Variations on the Transformer architecture | paper-stated |  |\n| d_k (big model) | inferred | Table 3: Variations on the Transformer architecture | inferred | 64 |\n| d_v (big model) | inferred | Table 3: Variations on the Transformer architecture | inferred | 64 |\n| P_drop (big model, EN-DE) | 0.3 | Table 3: Variations on the Transformer architecture | paper-stated |  |\n| P_drop (big model, EN-FR) | 0.1 | 6.1 Machine Translation | paper-stated |  |\n| N (Constituency Parsing) | 4 | 6.3 English Constituency Parsing | paper-stated |  |\n| d_model (Constituency Parsing) | 1024 | 6.3 English Constituency Parsing | paper-stated |  |\n| WSJ-only Vocabulary Size (Parsing) | 16000 | 6.3 English Constituency Parsing | paper-stated |  |\n| Semi-supervised Vocabulary Size (Parsing) | 32000 | 6.3 English Constituency Parsing | paper-stated |  |\n| Beam Size (Parsing) | 21 | 6.3 English Constituency Parsing | paper-stated |  |\n| Length Penalty alpha (Parsing) | 0.3 | 6.3 English Constituency Parsing | paper-stated |  |\n| Max Output Length (Inference) | input length + 50 | 6.1 Machine Translation | paper-stated |  |\n| Max Output Length (Parsing) | input length + 300 | 6.3 English Constituency Parsing | paper-stated |  |"
      }
    ],
    "downloads": [
      {
        "label": "architecture_summary.md",
        "href": "/demo-artifacts/transformer/architecture_summary.md"
      },
      {
        "label": "annotated_code.py",
        "href": "/demo-artifacts/transformer/annotated_code.py"
      },
      {
        "label": "hyperparameters.csv",
        "href": "/demo-artifacts/transformer/hyperparameters.csv"
      },
      {
        "label": "ambiguity_report.md",
        "href": "/demo-artifacts/transformer/ambiguity_report.md"
      }
    ]
  },
  "resnet": {
    "slug": "resnet",
    "title": "ResNet",
    "paperTitle": "Deep Residual Learning for Image Recognition",
    "summary": "Before this paper, it was widely believed that making neural networks deeper would always improve their ability to understand complex data like images.",
    "badges": [
      "residual blocks",
      "optimization",
      "implementation order"
    ],
    "sections": [
      {
        "id": "problem",
        "title": "1. What It Does",
        "content": "Before this paper, it was widely believed that making neural networks deeper would always improve their ability to understand complex data like images. However, researchers encountered a surprising problem: beyond a certain depth, adding more layers actually made the network perform *worse*, even on the data it was trained on. This wasn't due to the network memorizing the training data (overfitting), but because it became incredibly difficult to train effectively, struggling to even learn simple \"do-nothing\" transformations. This paper introduced a groundbreaking solution called \"residual learning.\" Instead of asking each block of layers to learn a completely new representation, it proposed that these layers should instead learn a *small adjustment* to their input, which is then simply added back to the original input using a direct \"shortcut connection.\" This clever trick made it much easier for the network to learn, especially when an optimal layer should ideally pass information through unchanged. The impact was profound: it enabled the successful training of neural networks with hundreds or even over a thousand layers, leading to significant leaps in image recognition accuracy and establishing a fundamental building block for nearly all advanced deep learning models today."
      },
      {
        "id": "mechanism",
        "title": "2. The Mechanism",
        "content": "### Briefing Section 2: The Mechanism\n\nThe core of the Deep Residual Learning framework is a reformulation of what the layers in a deep network are asked to learn. Instead of learning a direct mapping from input to output, the network is trained to learn a residual-the difference between the desired output and the input. This is achieved through a simple but powerful architectural element: the shortcut connection.\n\n#### 2.1 Residual Learning Formulation\n\nThe mechanism is motivated by the degradation problem, where deeper networks show higher training error than their shallower counterparts. This suggests that it is difficult for standard optimizers to learn an identity mapping (where the output is identical to the input) through a stack of non-linear layers, even if that is the optimal solution for the added layers.\n\nTo address this, the framework reframes the learning objective. Consider a block of layers that we want to learn an underlying mapping, denoted as `H(x)`. Instead of learning `H(x)` directly, the layers are tasked with learning a residual function `F(x)`, defined as:\n\n`F(x) := H(x) - x`\n\nHere, `x` represents the input to the block of layers, and `H(x)` is the desired output mapping for that block. By rearranging this definition, the original desired mapping `H(x)` is recovered by adding the input `x` back to the output of the residual function:\n\n`H(x) = F(x) + x`\n\nThe hypothesis is that it is easier for an optimizer to learn the residual `F(x)` than the original mapping `H(x)` (el-44). In the extreme case where an identity mapping is optimal (`H(x) = x`), the optimizer can simply drive the weights of the layers learning `F(x)` to zero. This is significantly easier than fitting an identity function through a complex stack of non-linear transformations like convolutional layers and ReLUs.\n\n#### 2.2 The Residual Block and Shortcut Connection\n\nThis `F(x) + x` formulation is implemented using a \"residual block\" containing a \"shortcut connection,\" as conceptually shown in the paper's Figure 2. A residual block consists of two paths:\n1.  A main path containing a few weighted layers (e.g., convolutional, Batch Normalization, ReLU) that learn the residual mapping `F(x)`.\n2.  A shortcut path that bypasses these layers and carries the input `x` forward.\n\nThe outputs of these two paths are then combined through element-wise addition. The output of the block, `y`, is formally defined as:\n\n**`y = F(x, {W_i}) + x`** (1)\n\nHere, we decode the symbols:\n*   `y` is the output vector of the residual block.\n*   `x` is the input vector to the block.\n*   `F(x, {W_i})` represents the residual mapping learned by the weighted layers in the main path.\n*   `{W_i}` denotes the set of weights and biases associated with these layers. For a typical two-layer block, `F` would be of the form `W_2 * σ(BN(W_1 * x))`, where `W_1` and `W_2` are weight matrices, `BN` is Batch Normalization, and `σ` is the ReLU activation function (el-53).\n\nThis identity shortcut `+ x` is the key component. It requires no additional parameters and adds negligible computational cost. Because it provides a direct, uninterrupted path for information and gradients to flow, it greatly simplifies the optimization of very deep networks. The entire architecture can be trained end-to-end with standard SGD and backpropagation.\n\n#### 2.3 Handling Dimension Mismatches\n\nThe element-wise addition in Equation (1) is only possible if the input `x` and the output of the residual function `F(x)` have the same dimensions (i.e., same height, width, and number of channels). However, in deep CNNs, it is common for convolutional layers to use a stride greater than one, which reduces spatial dimensions, or to increase the number of feature maps (channels).\n\nTo handle these dimension mismatches, the identity shortcut is replaced with a linear projection shortcut. The formulation becomes:\n\n**`y = F(x, {W_i}) + W_s * x`** (2)\n\nThe new symbol is:\n*   `W_s` is a projection matrix, implemented as a 1x1 convolution. Its purpose is solely to match the dimensions of `x` to the dimensions of `F(x)`. For example, if a block halves the spatial resolution and doubles the number of channels, `W_s` would be a 1x1 convolution with a stride of 2 and twice the number of output channels as input channels.\n\nThis projection shortcut introduces new parameters but is only used when necessary to align dimensions. The paper's experiments show that parameter-free identity shortcuts (Equation 1) are the most effective and are sufficient to solve the degradation problem, with projection shortcuts serving as a pragmatic solution for changes in dimensionality."
      },
      {
        "id": "prereqs",
        "title": "3. Prerequisites",
        "content": "Here is a dependency-ordered list of concepts foundational to understanding the paper \"Deep Residual Learning for Image Recognition\":\n\n### 1. Convolutional Neural Networks (CNNs)\n1) **Problem**: Standard fully-connected neural networks (MLPs) are inefficient for high-dimensional data like images. They have a massive number of parameters, leading to overfitting, and they do not account for the spatial structure (e.g., locality of pixels) in images.\n2) **Solution**: CNNs use specialized layers. Convolutional layers apply learnable filters across the image, sharing weights to detect features regardless of their location. Pooling layers downsample the feature maps, making the representation more robust to small translations. This creates a hierarchy of increasingly complex spatial features.\n3) **Usage in this paper**: The ResNet architecture is a very deep Convolutional Neural Network. It is built from stacks of convolutional layers (with 3x3 and 1x1 filters), batch normalization, and ReLU activations, designed for the task of image recognition.\n\n### 2. Deep Neural Networks\n1) **Problem**: Shallow neural networks have a limited capacity to represent complex functions. To solve challenging tasks like image recognition, models need to learn a rich hierarchy of features, from simple edges to complex objects.\n2) **Solution**: By stacking many layers, a deep neural network can learn features at various levels of abstraction. Each layer learns to represent the features from the previous layer in a more abstract way, increasing the model's expressive power.\n3) **Usage in this paper**: The entire paper is motivated by the desire to train *deeper* networks. The authors push the depth to unprecedented levels (152 layers and even over 1000 layers) to show that their residual learning framework overcomes the barriers that previously prevented such deep models from being trained effectively.\n\n### 3. Backpropagation\n1) **Problem**: To train a neural network, we need to calculate the gradient of a loss function with respect to every weight in the network. For a deep network with millions of parameters, doing this naively is computationally intractable.\n2) **Solution**: Backpropagation is an efficient algorithm for computing these gradients. It uses the chain rule of calculus to iteratively propagate the gradient from the final layer backward through the network, layer by layer, calculating the gradient for each weight along the way.\n3) **Usage in this paper**: Backpropagation is the fundamental algorithm used to train all the ResNet models. The paper confirms that the networks can be trained end-to-end by SGD with backpropagation.\n\n### 4. Stochastic Gradient Descent (SGD)\n1) **Problem**: Calculating the gradient of the loss function using the entire training dataset (batch gradient descent) is very slow and memory-intensive for large datasets. It can also get stuck in sharp local minima.\n2) **Solution**: SGD approximates the true gradient by computing it on a small, random subset of the data called a mini-batch. This is much faster, requires less memory, and the noise introduced by the mini-batch sampling can help the optimizer escape local minima and find better solutions.\n3) **Usage in this paper**: All models in the paper are trained using SGD with a momentum term. The mini-batch size is specified as 256 for ImageNet and 128 for CIFAR-10.\n\n### 5. Activation Functions (e.g., ReLU)\n1) **Problem**: Traditional activation functions like sigmoid and tanh suffer from the \"vanishing gradient problem\" in deep networks. Their gradients approach zero for large positive or negative inputs, which means that during backpropagation, the gradient signal can become too small to effectively update the weights in earlier layers, stalling the training process.\n2) **Solution**: The Rectified Linear Unit (ReLU), defined as f(x) = max(0, x), is a non-saturating activation function. Its gradient is 1 for all positive inputs, which helps maintain a strong gradient signal during backpropagation, leading to faster and more effective training of deep networks.\n3) **Usage in this paper**: ReLU is used as the non-linear activation function (denoted by σ) within the residual building blocks, typically after a batch normalization layer.\n\n### 6. Batch Normalization\n1) **Problem**: During training, the distribution of each layer's inputs changes as the parameters of the preceding layers are updated. This phenomenon, called \"internal covariate shift,\" slows down training because the network has to constantly adapt to these changing distributions. It also makes the network highly sensitive to weight initialization.\n2) **Solution**: Batch Normalization normalizes the output of a previous layer before it is fed to the next. For each mini-batch, it standardizes the activations to have zero mean and unit variance, and then applies a learnable scale and shift. This stabilizes the input distributions, allowing for higher learning rates and making the network less sensitive to initialization.\n3) **Usage in this paper**: Batch Normalization is a critical component of the ResNet architecture. It is applied right after each convolution and before the ReLU activation. The authors note that BN helps address the vanishing gradient problem, allowing them to focus on the separate degradation problem.\n\n### 7. Vanishing/Exploding Gradients\n1) **Problem**: In very deep networks, as the gradient is backpropagated from the output layer to the input layer, it is repeatedly multiplied by the weights of each layer. If these weights are small, the gradient can shrink exponentially (vanish), preventing early layers from learning. If the weights are large, the gradient can grow exponentially (explode), causing unstable training.\n2) **Solution**: This problem is addressed by a combination of techniques: careful weight initialization (e.g., He or Xavier initialization), non-saturating activation functions (e.g., ReLU), and intermediate normalization layers (e.g., Batch Normalization). Shortcut connections also provide a more direct path for the gradient to flow.\n3) **Usage in this paper**: The paper argues that the degradation problem they address is distinct from the vanishing gradient problem, which they state has been 'largely addressed' by techniques like Batch Normalization, which they use extensively.\n\n### 8. Identity Mapping / Shortcut Connections\n1) **Problem**: The degradation problem shows that it is difficult for a stack of non-linear layers to learn an identity mapping (i.e., a function where the output is simply the input). If a shallower network is optimal, a deeper network should be able to perform at least as well by learning identity functions for the extra layers, but in practice, optimizers fail to find this solution.\n2) **Solution**: Shortcut (or skip) connections provide a direct path for data to bypass one or more layers. An identity shortcut adds the input `x` to the output of the layers `F(x)`, resulting in `F(x) + x`. If the identity mapping is optimal, the network can easily achieve this by learning to make `F(x)` zero, which is easier than fitting an identity function with non-linear layers.\n3) **Usage in this paper**: This is the core mechanism of the proposed residual learning framework. Every residual block contains an identity shortcut connection that adds the block's input to its output, enabling the successful training of extremely deep networks."
      },
      {
        "id": "implementation",
        "title": "4. Implementation Map",
        "content": "### Implementation-Oriented Walkthrough\n\nThe generated implementation map below is rendered exactly as code, preserving assumptions and provenance markers.\n\n```python\n# Component: Batch Normalization\n# Provenance: paper-stated\n# Assumption: Implementation for 2D inputs (N, C, H, W) as it's common in vision tasks. If a different input dimension was intended (e.g., 1D or 3D), the mean/var dimensions would change.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CustomBatchNorm2d(nn.Module):\n    \"\"\"\n    Custom implementation of Batch Normalization for 2D inputs (e.g., images).\n    This module applies Batch Normalization over a mini-batch of 2D inputs.\n    \"\"\"\n    # ASSUMED: Implementation for 2D inputs (N, C, H, W) as it's common in vision tasks.\n    # If a different input dimension was intended (e.g., 1D or 3D), the mean/var dimensions would change.\n\n    def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True):\n        \"\"\"\n        Initializes the Batch Normalization layer.\n\n        Args:\n            num_features (int): Number of features (channels) in the input.\n            eps (float): A small value added to the variance to avoid division by zero.\n                         # INFERRED: Standard default value in PyTorch's BatchNorm.\n            momentum (float): The value used for the running_mean and running_var computation.\n                              # INFERRED: Standard default value in PyTorch's BatchNorm.\n            affine (bool): If True, this module has learnable affine parameters (gamma and beta).\n                           # INFERRED: Standard practice to include learnable scale (gamma) and shift (beta).\n            track_running_stats (bool): If True, tracks the running mean and variance.\n                                        # INFERRED: Standard practice to track running statistics for inference.\n        \"\"\"\n        super(CustomBatchNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n\n        if self.affine:\n            # Learnable scale parameter (gamma)\n            self.weight = nn.Parameter(torch.ones(num_features))\n            # Learnable shift parameter (beta)\n            self.bias = nn.Parameter(torch.zeros(num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n\n        if self.track_running_stats:\n            # Buffers for running statistics, not updated by backprop\n            self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            # Counter for number of batches processed, used for unbiased updates in some cases\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n        else:\n            self.register_buffer('running_mean', None)\n            self.register_buffer('running_var', None)\n            self.register_buffer('num_batches_tracked', None)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for Batch Normalization.\n\n        Args:\n            input (torch.Tensor): Input tensor of shape (N, C, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor after Batch Normalization.\n        \"\"\"\n        # Determine whether to use batch statistics or running statistics\n        if self.training and self.track_running_stats:\n            # Training mode with running stats tracking: calculate batch stats and update running stats\n            batch_mean = input.mean([0, 2, 3]) # Eq. (1)\n            batch_var = input.var([0, 2, 3], unbiased=True) # Eq. (2)\n\n            # Update running mean and variance using exponential moving average\n            # INFERRED: Standard update rule for running statistics in PyTorch's BatchNorm.\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n            self.num_batches_tracked += 1\n\n            current_mean = batch_mean\n            current_var = batch_var\n        elif not self.training and self.track_running_stats:\n            # Evaluation mode with running stats tracking: use tracked running stats\n            current_mean = self.running_mean\n            current_var = self.running_var\n        elif not self.track_running_stats:\n            # If not tracking running stats (e.g., like InstanceNorm behavior), always use batch stats\n            # INFERRED: This behavior aligns with PyTorch's BatchNorm when track_running_stats=False.\n            batch_mean = input.mean([0, 2, 3])\n            batch_var = input.var([0, 2, 3], unbiased=True)\n            current_mean = batch_mean\n            current_var = batch_var\n        else:\n            # This case should ideally not be reached with the above conditions.\n            # It would imply self.training is True but track_running_stats is False, which is covered by the last 'elif' branch.\n            # For robustness, could raise an error or default to batch stats.\n            raise RuntimeError(\"Unexpected state in BatchNorm forward pass.\")\n\n\n        # Normalize input: x_hat = (x - mu_B) / sqrt(sigma_B^2 + epsilon)\n        # The current_mean and current_var are (C,) tensors.\n        # We need to reshape them to (1, C, 1, 1) for broadcasting across (N, C, H, W) input.\n        normalized_input = (input - current_mean.view(1, -1, 1, 1)) / torch.sqrt(current_var.view(1, -1, 1, 1) + self.eps) # Eq. (3)\n\n        # Scale and shift: y = gamma * x_hat + beta\n        if self.affine:\n            # weight (gamma) and bias (beta) are (C,) tensors.\n            # Reshape to (1, C, 1, 1) for broadcasting.\n            output = self.weight.view(1, -1, 1, 1) * normalized_input + self.bias.view(1, -1, 1, 1) # Eq. (4)\n        else:\n            output = normalized_input\n\n        return output\n\n# Component: Convolutional Layer\n# Provenance: inferred\n# Assumption: The default value for `bias` is set to `False` when `None` is passed, based on ambiguity resolution A04, which states that convolutional layers followed by Batch Normalization should not include bias terms. The paper implies BN is used after each convolution.\n# Assumption: The 'n_in' mentioned in ambiguity resolution A03 for weight initialization is interpreted as 'fan_in' for convolutional layers, which is `in_channels * kernel_height * kernel_width`. PyTorch's `kaiming_normal_` with `mode='fan_in'` and `nonlinearity='relu'` is used to implement this He initialization.\n# Assumption: Bias terms, if present (i.e., if `bias=True` was explicitly passed), are initialized to zero.\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\nclass ConvolutionalLayer(nn.Module):\n    \"\"\"\n    Implementation for a Convolutional Layer.\n    \"\"\"\n    def __init__(self,\n                 in_channels: int,\n                 out_channels: int,\n                 kernel_size: int,\n                 stride: int = 1,\n                 padding: int = 0,\n                 dilation: int = 1,\n                 groups: int = 1,\n                 bias: bool = None):\n        \"\"\"\n        Initializes the ConvolutionalLayer.\n\n        Args:\n            in_channels (int): Number of channels in the input image.\n            out_channels (int): Number of channels produced by the convolution.\n            kernel_size (int): Size of the convolving kernel.\n            stride (int, optional): Stride of the convolution. Defaults to 1.\n            padding (int, optional): Zero-padding added to both sides of the input. Defaults to 0.\n            dilation (int, optional): Spacing between kernel elements. Defaults to 1.\n            groups (int, optional): Number of blocked connections from input channels to output channels. Defaults to 1.\n            bias (bool, optional): If True, adds a learnable bias to the output.\n                                   Defaults to None, which infers False based on A04.\n        \"\"\"\n        super().__init__()\n\n        # INFERRED: Default stride, padding, dilation, groups are standard for nn.Conv2d.\n        # ASSUMED: If bias is not explicitly provided, it defaults to False based on A04.\n        # A04: \"Do not include bias terms in convolutional or fully-connected layers that are followed by a Batch Normalization layer. The paper states BN is used after each convolution.\"\n        if bias is None:\n            bias = False # INFERRED: Based on A04, BN is used after each convolution, so bias is typically False.\n\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias\n        )\n\n        # A03: Weight Initialization\n        # \"Implement the weight initialization from [13]. For a given layer, the weights should be drawn from a zero-mean Gaussian distribution with a standard deviation of sqrt(2 / n_in), where n_in is the number of input units to the layer.\"\n        # For a convolutional layer, 'n_in' (or 'fan_in') is typically `in_channels * kernel_height * kernel_width`.\n        # PyTorch's `kaiming_normal_` with `mode='fan_in'` and `nonlinearity='relu'` correctly implements this\n        # for ReLU activations, which is a common pairing for He initialization.\n        init.kaiming_normal_(self.conv.weight, mode='fan_in', nonlinearity='relu') # Eq. (N) - Refers to [13] via A03\n        # ASSUMED: The 'n_in' in A03 refers to 'fan_in' for convolutional layers, which is in_channels * kernel_height * kernel_width.\n        # INFERRED: PyTorch's `kaiming_normal_` with `mode='fan_in'` and `nonlinearity='relu'` correctly implements this.\n\n        if self.conv.bias is not None:\n            init.constant_(self.conv.bias, 0) # ASSUMED: Bias terms, if present, are initialized to zero.\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the forward pass of the convolutional layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after convolution.\n        \"\"\"\n        return self.conv(x)\n\n# Component: Data Augmentation\n# Provenance: inferred\n# Assumption: AlexNetColorAugmentation: eigvecs and eigvals are pre-computed from ImageNet training set. These values are not provided in the prompt and cannot be computed here.\n# Assumption: AlexNetColorAugmentation: Input tensor to __call__ is already normalized to [0, 1] by transforms.ToTensor().\n# Assumption: get_data_augmentation_transforms: initial_resize_size (e.g., 256) is a common practice for ImageNet for both training (implicitly handled by RandomResizedCrop) and validation/test (explicitly used for Resize).\n# Assumption: get_data_augmentation_transforms: pca_eigvecs and pca_eigvals are pre-computed from ImageNet training set.\nimport torch\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom PIL import Image # PIL is used by torchvision transforms, so keep it for context\n\nclass AlexNetColorAugmentation(object):\n    \"\"\"\n    Implements the color augmentation described in the AlexNet paper [21].\n    Performs PCA on RGB pixel values and adds multiples of principal components.\n    This transform expects a torch.Tensor input of shape (C, H, W) with pixel values\n    in the range [0, 1].\n    \"\"\"\n    def __init__(self, eigvecs, eigvals, alpha_std=0.1):\n        \"\"\"\n        Args:\n            eigvecs (torch.Tensor or np.ndarray): Pre-computed eigenvectors from PCA on ImageNet RGB pixels.\n                                                  Shape: (3, 3)\n            eigvals (torch.Tensor or np.ndarray): Pre-computed eigenvalues from PCA on ImageNet RGB pixels.\n                                                  Shape: (3,)\n            alpha_std (float): Standard deviation for the Gaussian random variable.\n                               INFERRED: 0.1 based on AlexNet paper [21] referenced by A02.\n        \"\"\"\n        # ASSUMED: eigvecs and eigvals are pre-computed from ImageNet training set.\n        # These values are not provided in the prompt and cannot be computed here.\n        if not isinstance(eigvecs, torch.Tensor):\n            self.eigvecs = torch.from_numpy(eigvecs).float()\n        else:\n            self.eigvecs = eigvecs.float()\n\n        if not isinstance(eigvals, torch.Tensor):\n            self.eigvals = torch.from_numpy(eigvals).float()\n        else:\n            self.eigvals = eigvals.float()\n\n        if self.eigvecs.shape != (3, 3) or self.eigvals.shape != (3,):\n            raise ValueError(\"eigvecs must be (3, 3) and eigvals must be (3,)\")\n\n        self.alpha_std = alpha_std\n\n    def __call__(self, img_tensor):\n        \"\"\"\n        Args:\n            img_tensor (torch.Tensor): Image to be color augmented.\n                                       Expected shape: (C, H, W) and pixel values in [0, 1].\n        Returns:\n            torch.Tensor: Color augmented image, shape (C, H, W), values clamped to [0, 1].\n        \"\"\"\n        if not isinstance(img_tensor, torch.Tensor):\n            raise TypeError(\"Input to AlexNetColorAugmentation must be a torch.Tensor.\")\n        if img_tensor.dim() != 3 or img_tensor.shape[0] != 3:\n            raise ValueError(\"Input Tensor must be of shape (C, H, W) with C=3.\")\n        if img_tensor.dtype != torch.float32:\n            img_tensor = img_tensor.float()\n        # ASSUMED: Input tensor is already normalized to [0, 1] by transforms.ToTensor()\n\n        # Convert to (H, W, C) for easier pixel manipulation\n        img_tensor_hwc = img_tensor.permute(1, 2, 0).clone() # (H, W, C)\n\n        # Reshape image to (N_pixels, 3) for PCA application\n        original_shape = img_tensor_hwc.shape\n        img_flat = img_tensor_hwc.view(-1, 3) # (H*W, 3)\n\n        # Generate random variables alpha_i from N(0, alpha_std)\n        # INFERRED: Standard deviation for Gaussian noise is 0.1 based on AlexNet paper [21] referenced by A02.\n        alphas = torch.randn(3, device=img_tensor.device) * self.alpha_std # (3,)\n\n        # Calculate the perturbation vector p = E * (alpha * lambda)\n        # where E are eigenvectors, lambda are eigenvalues\n        # Eq. (A02 description)\n        perturbation = torch.matmul(self.eigvecs.to(img_tensor.device), alphas * self.eigvals.to(img_tensor.device)) # (3, 3) * (3,) -> (3,)\n\n        # Add perturbation to each pixel\n        # Clamp to [0, 1] range after adding perturbation\n        # Eq. (A02 description)\n        augmented_img_flat = img_flat + perturbation\n        augmented_img_flat = torch.clamp(augmented_img_flat, 0.0, 1.0)\n\n        # Reshape back to original image dimensions (H, W, C)\n        augmented_img_hwc = augmented_img_flat.view(original_shape)\n\n        # Convert back to (C, H, W) for consistency with torchvision transforms\n        return augmented_img_hwc.permute(2, 0, 1)\n\ndef get_data_augmentation_transforms(\n    is_train: bool,\n    image_size: int = 224,\n    initial_resize_size: int = 256, # ASSUMED: Common practice for ImageNet.\n    normalize_mean: list = None,\n    normalize_std: list = None,\n    pca_eigvecs: np.ndarray = None, # Placeholder for pre-computed PCA components\n    pca_eigvals: np.ndarray = None, # Placeholder for pre-computed PCA components\n    color_augmentation_std: float = 0.1 # INFERRED: 0.1 based on AlexNet paper [21] referenced by A02.\n):\n    \"\"\"\n    Generates torchvision transforms for data augmentation.\n\n    Args:\n        is_train (bool): If True, applies training augmentations (random crop, flip, color jitter).\n                         If False, applies validation/test augmentations (center crop).\n        image_size (int): The final size of the image after cropping (e.g., 224 for ImageNet).\n        initial_resize_size (int): For validation/test, the shortest side of the image is resized to this.\n                                   For training, RandomResizedCrop handles resizing internally.\n                                   ASSUMED: 256 for ImageNet, as per common practice.\n        normalize_mean (list): Mean values for image normalization (e.g., [0.485, 0.456, 0.406] for ImageNet).\n                               If None, normalization is skipped.\n        normalize_std (list): Standard deviation values for image normalization (e.g., [0.229, 0.224, 0.225] for ImageNet).\n                              If None, normalization is skipped.\n        pca_eigvecs (np.ndarray): Pre-computed eigenvectors for AlexNet-style color augmentation.\n                                  Shape (3, 3). Required if color augmentation is desired.\n                                  ASSUMED: Pre-computed from ImageNet training set.\n        pca_eigvals (np.ndarray): Pre-computed eigenvalues for AlexNet-style color augmentation.\n                                  Shape (3,). Required if color augmentation is desired.\n                                  ASSUMED: Pre-computed from ImageNet training set.\n        color_augmentation_std (float): Standard deviation for the Gaussian random variable\n                                        used in AlexNet-style color augmentation.\n                                        INFERRED: 0.1 based on AlexNet paper [21] referenced by A02.\n\n    Returns:\n        torchvision.transforms.Compose: A composition of data augmentation transforms.\n    \"\"\"\n    transform_list = []\n\n    if is_train:\n        # \"randomly crop a 224x224 region from an image or its horizontal flip\"\n        # RandomResizedCrop handles both resizing and cropping to the target size.\n        transform_list.append(transforms.RandomResizedCrop(image_size))\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n        # A02: Implement AlexNet-style color augmentation\n        if pca_eigvecs is not None and pca_eigvals is not None:\n            transform_list.append(transforms.ToTensor()) # Convert to Tensor (C, H, W) for custom transform\n            transform_list.append(AlexNetColorAugmentation(pca_eigvecs, pca_eigvals, color_augmentation_std))\n            # AlexNetColorAugmentation now guarantees (C, H, W) output.\n        else:\n            # If no PCA color augmentation, convert to Tensor here\n            transform_list.append(transforms.ToTensor())\n\n    else:\n        # For validation/testing\n        # Resize shortest side to initial_resize_size, then center crop to image_size.\n        transform_list.append(transforms.Resize(initial_resize_size)) # ASSUMED: Resize shortest side to 256 for validation/test\n        transform_list.append(transforms.CenterCrop(image_size))\n        transform_list.append(transforms.ToTensor())\n\n    if normalize_mean is not None and normalize_std is not None:\n        transform_list.append(transforms.Normalize(mean=normalize_mean, std=normalize_std))\n\n    return transforms.Compose(transform_list)\n```"
      },
      {
        "id": "ambiguity",
        "title": "5. Missing Details",
        "content": "## A01: ImageNet Learning Rate Schedule Trigger\n- Type: missing_hyperparameter\n- Section: 3.4. Implementation\n- Ambiguous point: The paper states the learning rate for ImageNet training is 'divided by 10 when the error plateaus'.\n- Implementation consequence: The term 'plateaus' is not defined. Without knowing the exact metric (training or validation error), the patience (number of epochs/iterations to wait), and the threshold for what constitutes a plateau, the learning rate schedule cannot be reproduced. This will lead to different convergence behavior and final model accuracy.\n- Agent resolution: A common implementation is to monitor the validation error and reduce the learning rate if it does not improve for a set number of epochs (e.g., 5-10 epochs). The CIFAR-10 experiments use a fixed iteration-based schedule, which is an alternative. Given the lack of detail, a fixed schedule like the one for CIFAR-10 (e.g., dropping at 300k and 500k iterations) would be a more reproducible choice.\n- Confidence: 0.9\n\n## A02: Standard Color Augmentation Details\n- Type: missing_training_detail\n- Section: 3.4. Implementation\n- Ambiguous point: The paper states 'The standard color augmentation in [21] is used.' for ImageNet training.\n- Implementation consequence: Reference [21] (Krizhevsky et al., 2012) describes a specific PCA-based color jittering technique. If a developer is unaware of this or implements a different color augmentation (e.g., simple brightness/contrast adjustments), the training data distribution will be different, which can affect the final model's accuracy and robustness.\n- Agent resolution: Implement the color augmentation as described in the AlexNet paper [21]. This involves performing PCA on the RGB pixel values of the ImageNet training set, and then for each image, adding multiples of the found principal components, with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from a Gaussian distribution.\n- Confidence: 1.0\n\n## A03: Weight Initialization Details\n- Type: missing_training_detail\n- Section: 3.4. Implementation\n- Ambiguous point: The paper states 'We initialize the weights as in [13]'.\n- Implementation consequence: Reference [13] (He et al., 2015, 'Delving Deep into Rectifiers') introduces a specific initialization method for ReLU networks (often called 'He initialization'). Using a different initialization, like Xavier/Glorot, could lead to slower convergence or prevent very deep networks from converging at all, as it is not specifically designed for ReLU nonlinearities.\n- Agent resolution: Implement the weight initialization from [13]. For a given layer, the weights should be drawn from a zero-mean Gaussian distribution with a standard deviation of sqrt(2 / n_in), where n_in is the number of input units to the layer.\n- Confidence: 1.0\n\n## A04: Use of Biases in Convolutional/FC Layers\n- Type: underspecified_architecture\n- Section: 3.2. Identity Mapping by Shortcuts\n- Ambiguous point: In Section 3.2, the formula for a residual block is given, and the text notes 'the biases are omitted for simplifying notations'. It is not explicitly stated whether biases are used in the actual implementation.\n- Implementation consequence: If biases are added to convolutional layers that are immediately followed by a Batch Normalization layer, the effect of the bias will be cancelled out by the mean subtraction step in BN. Adding them would add useless parameters to the model, slightly increasing memory usage and computation for no benefit. If BN were not present, omitting biases would be a significant architectural change.\n- Agent resolution: Do not include bias terms in convolutional or fully-connected layers that are followed by a Batch Normalization layer. The paper states BN is used after each convolution. The final FC layer before the softmax does not have a subsequent BN layer and should include a bias term.\n- Confidence: 0.95\n\n## A05: Exact Projection Shortcut Implementation\n- Type: underspecified_architecture\n- Section: 3.3. Network Architectures\n- Ambiguous point: For projection shortcuts (Option B), the paper states they are done by 1x1 convolutions to match dimensions. When crossing feature maps of two sizes, they are performed with a stride of 2. It is not specified if these 1x1 convolutions have a subsequent BN and/or ReLU.\n- Implementation consequence: If the projection shortcut path includes BN and ReLU, its statistical properties and non-linearity will be different from a simple linear projection. This could affect how information propagates through the shortcut and impact training dynamics. Most open-source implementations use a 1x1 convolution without any non-linearity or normalization on the shortcut path.\n- Agent resolution: The projection shortcut should consist of only a 1x1 convolutional layer with a stride of 2. It should not be followed by Batch Normalization or a ReLU activation. This preserves the shortcut as a linear projection to match dimensions, which is its stated purpose.\n- Confidence: 0.9\n\n## A06: Composition of the 6-Model Ensemble\n- Type: missing_training_detail\n- Section: 4.1. ImageNet Classification\n- Ambiguous point: For the best ImageNet result, the paper mentions 'We combine six models of different depth to form an ensemble (only with two 152-layer ones at the time of submitting)'.\n- Implementation consequence: The final state-of-the-art result of 3.57% top-5 error cannot be reproduced without knowing the exact architecture of the other four models in the ensemble. The performance of an ensemble is highly dependent on the diversity and individual performance of its constituent models.\n- Agent resolution: This result is not reproducible from the paper alone. To create a similar ensemble, one could train one of each of the other architectures presented (e.g., ResNet-34, ResNet-50, ResNet-101) and a sixth model, perhaps another ResNet-152 with a different random seed or a ResNet-101. The final performance will likely differ.\n- Confidence: 1.0"
      },
      {
        "id": "training",
        "title": "6. Training Recipe",
        "content": "### Hyperparameter Registry\n\n| name | value | source | status | suggested_default |\n| --- | --- | --- | --- | --- |\n| ImageNet: Image Resizing (Shorter Side) | [256, 480] | 3.4. Implementation | paper-stated |  |\n| ImageNet: Crop Size | 224x224 | 3.4. Implementation | paper-stated |  |\n| ImageNet: Optimizer | SGD | 3.4. Implementation | paper-stated |  |\n| ImageNet: Mini-batch Size | 256 | 3.4. Implementation | paper-stated |  |\n| ImageNet: Initial Learning Rate | 0.1 | 3.4. Implementation | paper-stated |  |\n| ImageNet: Learning Rate Schedule | divided by 10 when the error plateaus | 3.4. Implementation | paper-stated |  |\n| ImageNet: Total Iterations | up to 60e4 | 3.4. Implementation | paper-stated |  |\n| ImageNet: Weight Decay | 0.0001 | 3.4. Implementation | paper-stated |  |\n| ImageNet: Momentum | 0.9 | 3.4. Implementation | paper-stated |  |\n| ImageNet: Dropout | not used | 3.4. Implementation | paper-stated |  |\n| ImageNet: Multi-scale Testing (Shorter Side) | {224, 256, 384, 480, 640} | 3.4. Implementation | paper-stated |  |\n| CIFAR-10: Input Size | 32x32 | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: Data Augmentation Padding | 4 pixels on each side | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: Data Augmentation Crop Size | 32x32 | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: Weight Decay | 0.0001 | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: Momentum | 0.9 | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: Dropout | not used | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: Mini-batch Size | 128 | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: Initial Learning Rate | 0.1 | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: Learning Rate Schedule | divide by 10 at 32k and 48k iterations | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: Total Iterations | 64k | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: ResNet-110 Warmup LR | 0.01 | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| CIFAR-10: ResNet-110 Warmup Duration | until training error is below 80% (about 400 iterations) | 4.2. CIFAR-10 and Analysis | paper-stated |  |\n| Object Detection: RPN Proposals | 300 | A. Object Detection Baselines | paper-stated |  |\n| Object Detection (COCO): RPN Mini-batch Size | 8 images | A. Object Detection Baselines | paper-stated |  |\n| Object Detection (COCO): Fast R-CNN Mini-batch Size | 16 images | A. Object Detection Baselines | paper-stated |  |\n| Object Detection (COCO): Initial Learning Rate | 0.001 | A. Object Detection Baselines | paper-stated |  |\n| Object Detection (COCO): Learning Rate Schedule | 0.001 for 240k iterations, then 0.0001 for 80k iterations | A. Object Detection Baselines | paper-stated |  |\n| Object Detection (Improvements): NMS IoU Threshold | 0.3 | B. Object Detection Improvements | paper-stated |  |\n| Object Detection (Improvements): Multi-scale Testing (Shorter Side) | {200, 400, 600, 800, 1000} | B. Object Detection Improvements | paper-stated |  |\n| Localization: Mini-batch Size | 256 | C. ImageNet Localization | paper-stated |  |\n| Localization: Anchor Sampling Ratio (Pos:Neg) | 1:1 | C. ImageNet Localization | paper-stated |  |\n| Localization: Anchors Sampled per Image | 8 | C. ImageNet Localization | paper-stated |  |"
      }
    ],
    "downloads": [
      {
        "label": "architecture_summary.md",
        "href": "/demo-artifacts/resnet/architecture_summary.md"
      },
      {
        "label": "annotated_code.py",
        "href": "/demo-artifacts/resnet/annotated_code.py"
      },
      {
        "label": "hyperparameters.csv",
        "href": "/demo-artifacts/resnet/hyperparameters.csv"
      },
      {
        "label": "ambiguity_report.md",
        "href": "/demo-artifacts/resnet/ambiguity_report.md"
      }
    ]
  },
  "bert": {
    "slug": "bert",
    "title": "BERT",
    "paperTitle": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "summary": "Previously, computer models designed to understand language could only process text in one direction, like reading a sentence from left to right.",
    "badges": [
      "pretraining",
      "fine-tuning",
      "nlp pipeline"
    ],
    "sections": [
      {
        "id": "problem",
        "title": "1. What It Does",
        "content": "Previously, computer models designed to understand language could only process text in one direction, like reading a sentence from left to right. This limitation meant they often struggled to fully grasp context, especially for tasks like question answering where understanding the entire sentence and its relationship to others is key. This paper introduces BERT, a novel method for training language models that can understand text by looking at words from both their left and right sides simultaneously. BERT learns this deep understanding through two clever training exercises: first, it predicts randomly hidden words within a sentence by considering all the words around them, and second, it determines if two sentences logically follow each other. This innovative bidirectional training allows BERT to develop a much richer and more complete understanding of language, enabling it to be easily adapted to excel at a wide variety of language tasks, such as answering questions or classifying text, and achieving significantly better performance than previous methods."
      },
      {
        "id": "mechanism",
        "title": "2. The Mechanism",
        "content": "### **Briefing Section 2: The Mechanism**\n\nThe BERT framework operates in two distinct phases: pre-training and fine-tuning. The core mechanism involves preparing a specialized input representation, processing it through a deep bidirectional Transformer encoder, and then using the output for either general language understanding tasks (pre-training) or specific downstream tasks (fine-tuning).\n\n#### **Step 1: Input Representation**\n\nTo handle a variety of downstream tasks, BERT requires a unified and rich input format that can represent either a single sentence or a pair of sentences (e.g., Question-Paragraph pairs). This is achieved by constructing an input embedding for each token from the sum of three distinct embeddings, a process visualized for fine-tuning tasks in Figure 4 `(el-364)`.\n\n1.  **Tokenization:** The input text is first tokenized using a WordPiece tokenizer `(Section 3)`, which breaks words into common sub-word units. This helps manage vocabulary size and handle out-of-vocabulary words. Two special tokens are added:\n    *   `[CLS]`: A special classification token inserted at the beginning of every sequence. Its final hidden state is used as the aggregate sequence representation for classification tasks.\n    *   `[SEP]`: A separator token used to distinguish between different sentences, such as separating a question from a paragraph in SQuAD.\n\n2.  **Embedding Summation:** Each token in the input sequence is converted into a vector by summing three learned embeddings:\n    *   **Token Embeddings:** These represent the meaning of the token itself, mapping each token in the 30,000-word vocabulary to a vector of hidden size `H` (e.g., 768 for BERT<sub>BASE</sub>).\n    *   **Segment Embeddings:** These are necessary to distinguish between sentences in a pair. A learned embedding for \"Sentence A\" is added to every token in the first sentence, and a learned embedding for \"Sentence B\" is added to every token in the second sentence. This is crucial for the Next Sentence Prediction pre-training task and for sentence-pair fine-tuning tasks like Question Answering, as shown in `Figure 4 (a, c)`.\n    *   **Position Embeddings:** The core self-attention mechanism of the Transformer is permutation-invariant, meaning it has no inherent sense of token order. To counteract this, learned position embeddings are added to each token to encode its position in the sequence. This is a key prerequisite for the model to understand sentence structure `(A01)`.\n\nThe resulting summed vector for each token serves as the input to the main model.\n\n#### **Step 2: Bidirectional Processing via Transformer Encoder**\n\nThe sequence of input embeddings is fed into the core of the BERT model: a multi-layer bidirectional Transformer encoder. The architecture consists of a stack of identical layers (L=12 for BERT<sub>BASE</sub>, L=24 for BERT<sub>LARGE</sub>) `(Section 3)`.\n\nEach layer transforms the sequence of vectors using two main sub-layers: a multi-head self-attention mechanism and a position-wise feed-forward network. The key innovation of BERT lies in its application of the self-attention mechanism.\n\n*   **Prerequisite: Self-Attention:** Self-attention allows the model to compute a token's representation by weighing the influence of all other tokens in the sequence.\n*   **BERT's Bidirectionality:** Unlike unidirectional models like GPT which mask future tokens (a left-to-right attention), BERT's self-attention mechanism allows every token to attend to every other token in the sequence, both to its left and right, in every layer. This is why BERT is described as \"deeply bidirectional.\" This architectural choice is necessary to build a comprehensive, context-aware representation of each token, which is critical for tasks that require a holistic understanding of the entire input.\n\nAfter passing through all `L` layers, the encoder outputs a sequence of final hidden states, `T_i ∈ R^H`, for each input token `i`. The final hidden state corresponding to the `[CLS]` token is denoted as `C ∈ R^H`. These output vectors are then used for the pre-training tasks.\n\n#### **Step 3: Unsupervised Pre-training**\n\nTo make the bidirectional encoder learn meaningful language representations, it is pre-trained on a large unlabeled corpus using two novel, simultaneous unsupervised tasks `(Section 3.1)`. The total training loss is the unweighted sum of the losses from these two tasks `(A07)`.\n\n1.  **Task #1: Masked Language Model (MLM)**\n    *   **Motivation:** A standard language model objective (predicting the next word) is inherently unidirectional. To train a bidirectional model, a different objective is needed.\n    *   **Mechanism:** 15% of the input tokens are randomly selected for prediction. Of these selected tokens:\n        *   80% are replaced with a special `[MASK]` token.\n        *   10% are replaced with a random token from the vocabulary.\n        *   10% are left unchanged.\n    This 80/10/10 strategy is necessary to mitigate the mismatch between pre-training, which sees `[MASK]` tokens, and fine-tuning, which does not. The model's objective is to predict the original token based on its final hidden state `T_i`, which is conditioned on the full, unmasked context from both directions. An `MLM Head` (a simple classification layer over the vocabulary) is placed on top of the Transformer's output to compute this prediction.\n\n2.  **Task #2: Next Sentence Prediction (NSP)**\n    *   **Motivation:** Many important downstream tasks, such as Question Answering (QA) and Natural Language Inference (NLI), require an understanding of the relationships between sentences. This is not directly captured by language modeling alone.\n    *   **Mechanism:** The model is presented with two sentences, A and B. For 50% of the training examples, B is the actual sentence that follows A in the original text; for the other 50%, B is a random sentence sampled from the corpus `(A03)`. The model must predict whether B is the true next sentence. This binary classification task is trained using the `[CLS]` token's final hidden state `C`, which is passed to a simple `NSP Head`. This forces the `[CLS]` representation to capture the relationship between the two input sentences.\n\n#### **Step 4: Adaptation for Fine-Tuning**\n\nOnce pre-training is complete, the MLM and NSP heads are discarded. The pre-trained BERT parameters provide a powerful starting point for a wide range of downstream tasks, requiring only the addition of a small, task-specific output layer. As illustrated in `Figure 4` `(el-364)`, the same pre-trained model can be adapted with minimal changes.\n\n*   **For Sentence-level Classification:** For tasks like sentiment analysis or NLI, a single linear classification layer is added on top of the BERT model. The final hidden state of the `[CLS]` token, `C`, is fed into this layer to produce classification logits, which are then passed through a softmax function `(Figure 4 a, b)`.\n\n*   **For Token-level Tasks (e.g., SQuAD Question Answering):** For tasks that require predicting a span of text, the final hidden states of all tokens, `T_i`, are used. As shown in `Figure 4 (c)`, two new vectors are introduced for fine-tuning: a start vector `S` and an end vector `E`. The probability of a token `i` being the start of the answer span is calculated as:\n\n    `P_i = e^(S · T_i) / Σ_j e^(S · T_j)` `(el-103)`\n\n    Where:\n    *   `P_i` is the probability of token `i` being the start of the answer.\n    *   `S ∈ R^H` is a learnable start-of-span vector.\n    *   `T_i ∈ R^H` is the final hidden state of the i-th token from BERT.\n    *   `S · T_i` is the dot product, yielding a scalar score for token `i` being the start.\n    *   The denominator is a softmax function that normalizes the scores for all tokens `j` in the paragraph into a probability distribution.\n\n    A similar calculation is performed with the end vector `E` to find the probability distribution for the end of the answer span. The model is trained to predict the correct start and end indices. This approach allows BERT to be effectively adapted for complex token-level tasks with minimal architectural modification."
      },
      {
        "id": "prereqs",
        "title": "3. Prerequisites",
        "content": "Here is a dependency-ordered list of concepts foundational to understanding the paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\":\n\n### Section 3: What You Need To Already Know\n\nThis section outlines the fundamental concepts and technologies that form the bedrock of BERT. Understanding these prerequisites is crucial for grasping BERT's innovations and its impact on Natural Language Processing.\n\n---\n\n#### 1. Word Embeddings / Tokenization\n\n1)  **Problem**\n    Neural networks require numerical inputs, but human language consists of discrete text. How can we convert words and sentences into a format that a machine learning model can process meaningfully?\n\n2)  **Solution**\n    **Tokenization** is the process of breaking down raw text into smaller units called tokens (which can be words, sub-words, or characters). Each unique token is then mapped to a dense numerical vector called an **embedding**. These embeddings are typically learned during training, allowing them to capture semantic relationships (e.g., words with similar meanings will have similar embedding vectors).\n\n3)  **Usage in this paper**\n    BERT uses a sub-word tokenizer called **WordPiece** to handle out-of-vocabulary words and manage vocabulary size. Its input representation for each token is the sum of three learned embeddings: a token embedding (representing the token itself), a segment embedding (indicating which sentence the token belongs to), and a position embedding (denoting its position within the sequence).\n\n#### 2. Unsupervised Learning\n\n1)  **Problem**\n    Training powerful machine learning models often requires vast amounts of labeled data, which is expensive, time-consuming, and often impractical to obtain for every possible task. How can we leverage the enormous quantities of readily available unlabeled data (like text on the internet) to build effective models?\n\n2)  **Solution**\n    **Unsupervised learning** involves training models on tasks that do not require explicit manual labels. Instead, the labels are generated directly from the input data itself, a technique often referred to as **self-supervised learning**. For example, in text, one can mask a word and ask the model to predict it, using the original word as the 'label'. This forces the model to learn underlying patterns and structures in the data.\n\n3)  **Usage in this paper**\n    BERT's entire pre-training phase is a prime example of unsupervised learning. Both the **Masked Language Model (MLM)** and **Next Sentence Prediction (NSP)** tasks generate their own labels from raw input text, enabling BERT to learn rich, general-purpose language representations from a massive unlabeled corpus without human annotation.\n\n#### 3. Language Modeling\n\n1)  **Problem**\n    How can we train a machine to learn the statistical patterns, grammatical rules, and semantic nuances of a language from raw text without explicit linguistic annotations or task-specific labels?\n\n2)  **Solution**\n    **Language modeling** is a fundamental task in NLP where a model is trained to predict the probability of a sequence of words. Traditionally, this involves predicting the *next* word in a sequence given the preceding words (e.g., given \"The cat sat on the\", predict \"mat\"). By performing this prediction task over vast amounts of text, the model implicitly learns about syntax, semantics, and context.\n\n3)  **Usage in this paper**\n    BERT introduces a novel approach to language modeling called the **Masked Language Model (MLM)**. Unlike traditional unidirectional language models, MLM randomly masks a percentage of input tokens and trains the model to predict the original masked tokens. This forces the model to learn context from both the left and right sides of a word simultaneously, leading to deeply bidirectional representations.\n\n#### 4. Self-Attention Mechanism\n\n1)  **Problem**\n    When interpreting a specific word in a sentence, its meaning is often heavily influenced by other words, potentially far away. For example, in \"The animal didn't cross the street because *it* was too tired,\" \"it\" refers to \"the animal.\" Traditional sequential models (like RNNs) struggle to efficiently capture these long-range dependencies and contextual relationships across an entire sentence.\n\n2)  **Solution**\n    **Self-attention** is a mechanism that allows a model to dynamically weigh the importance of all other words in an input sequence when processing a single word. For each word, it computes 'attention scores' against every other word in the sequence (including itself). The final representation of the word is then a weighted sum of all word representations, where the weights are determined by these attention scores. This enables the model to focus on the most relevant contextual words, regardless of their position.\n\n3)  **Usage in this paper**\n    Self-attention is the fundamental building block of each Transformer layer within BERT. Because it allows each token to attend to all other tokens in the input sequence (both left and right), it is the core mechanism that enables BERT to create its \"deeply bidirectional\" representations, crucial for understanding context from all directions.\n\n#### 5. Transformer Architecture\n\n1)  **Problem**\n    Recurrent Neural Networks (RNNs) and their variants (LSTMs, GRUs) process sequential data one token at a time. This sequential nature makes them slow to train (due to limited parallelization) and inherently difficult to capture dependencies between words that are very far apart in a sentence, as information can degrade over long sequences.\n\n2)  **Solution**\n    The **Transformer architecture** overcomes these limitations by completely eschewing recurrence and convolutions. It processes all tokens in a sequence simultaneously using the **self-attention mechanism**. This allows for massive parallelization during training, significantly speeding up computation. By directly relating any two words in the sequence through self-attention, regardless of their distance, the Transformer effectively captures long-range dependencies.\n\n3)  **Usage in this paper**\n    BERT's core architecture is a multi-layer **Transformer encoder**. The paper leverages the Transformer's ability to process sequences in parallel and its powerful self-attention mechanism to create deep bidirectional representations, which are essential for its state-of-the-art performance across various NLP tasks.\n\n#### 6. Pre-training and Fine-tuning Paradigm\n\n1)  **Problem**\n    Training very large, complex neural networks (deep learning models) from scratch for every specific NLP task (like sentiment analysis, question answering, or named entity recognition) requires an immense amount of task-specific labeled data. This data is often expensive, time-consuming, and difficult to obtain, leading to a bottleneck in developing high-performing models for diverse applications.\n\n2)  **Solution**\n    This paradigm involves two distinct stages. First, a large model is **'pre-trained'** on a massive, easily available unlabeled dataset (e.g., all of Wikipedia and BooksCorpus) using general, self-supervised tasks (like language modeling). This process teaches the model a broad, general 'understanding' of the language's structure, semantics, and context. Second, this pre-trained model is then **'fine-tuned'** by continuing its training on a much smaller, task-specific labeled dataset. During fine-tuning, a small task-specific output layer is added, and all parameters (both the original pre-trained ones and the new layer) are updated with a low learning rate, adapting the model's general knowledge to the specific downstream task.\n\n3)  **Usage in this paper**\n    This two-stage methodology is the core contribution and operational principle of BERT. The model is pre-trained on the **Masked Language Model (MLM)** and **Next Sentence Prediction (NSP)** tasks using a vast text corpus. Subsequently, this single pre-trained BERT model is fine-tuned with minimal architectural changes on 11 different downstream NLP tasks, achieving new state-of-the-art results and demonstrating the power of this transfer learning approach."
      },
      {
        "id": "implementation",
        "title": "4. Implementation Map",
        "content": "### Implementation-Oriented Walkthrough\n\nThe generated implementation map below is rendered exactly as code, preserving assumptions and provenance markers.\n\n```python\n# Component: BERT Model (Multi-layer Transformer Encoder)\n# Provenance: paper-stated\n# Assumption: A01: Position embeddings are implemented as a learned lookup table of size (max_sequence_length, hidden_size), as specified in the resolved ambiguity. They are trained from scratch.\n# Assumption: The specific values for hyperparameters like `vocab_size`, `hidden_size`, `num_attention_heads`, `num_hidden_layers`, `intermediate_size`, `max_position_embeddings`, `type_vocab_size`, `hidden_dropout_prob`, `attention_probs_dropout_prob`, and `layer_norm_eps` are not provided in the prompt. The code is designed to accept these as arguments. For a typical BERT-base model, these would be, for example: `vocab_size=30522`, `hidden_size=768`, `num_attention_heads=12`, `num_hidden_layers=12`, `intermediate_size=3072`, `max_position_embeddings=512`, `type_vocab_size=2`, `hidden_dropout_prob=0.1`, `attention_probs_dropout_prob=0.1`, `layer_norm_eps=1e-12`.\nimport torch\nimport torch.nn as nn\nimport math\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"\n    Construct the embeddings from word, position and token_type embeddings.\n    \"\"\"\n    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, hidden_dropout_prob, layer_norm_eps):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n        # ASSUMED: A01 - Position embeddings are a learned lookup table.\n        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n\n        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(hidden_dropout_prob)\n\n        # INFERRED: Position IDs are usually generated internally if not provided.\n        # This is a common practice in HuggingFace Transformers based on the original BERT implementation.\n        self.register_buffer(\"position_ids\", torch.arange(max_position_embeddings).expand((1, -1)))\n\n    def forward(self, input_ids=None, token_type_ids=None, position_ids=None):\n        if input_ids is None:\n            # TODO: Handle case where input_ids is None, perhaps raise an error or return zero embeddings.\n            # For now, assume input_ids is always provided.\n            raise ValueError(\"input_ids must be provided for BertEmbeddings.\")\n\n        input_shape = input_ids.size()\n        seq_length = input_shape[1]\n\n        if position_ids is None:\n            # INFERRED: If position_ids are not provided, generate them based on sequence length.\n            position_ids = self.position_ids[:, :seq_length]\n\n        if token_type_ids is None:\n            # INFERRED: If token_type_ids are not provided, assume all zeros (segment A).\n            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=input_ids.device)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob):\n        super().__init__()\n        if hidden_size % num_attention_heads != 0:\n            raise ValueError(\n                f\"The hidden size ({hidden_size}) is not a multiple of the number of attention \"\n                f\"heads ({num_attention_heads})\"\n            )\n\n        self.num_attention_heads = num_attention_heads\n        self.attention_head_size = hidden_size // num_attention_heads\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(hidden_size, self.all_head_size)\n        self.key = nn.Linear(hidden_size, self.all_head_size)\n        self.value = nn.Linear(hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3) # (batch_size, num_heads, seq_len, head_size)\n\n    def forward(self, hidden_states, attention_mask=None):\n        query_layer = self.query(hidden_states)\n        key_layer = self.key(hidden_states)\n        value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(query_layer)\n        key_layer = self.transpose_for_scores(key_layer)\n        value_layer = self.transpose_for_scores(value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # Eq. (1)\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # Eq. (1) scaling\n        if attention_mask is not None:\n            # Apply the attention mask (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1) # Eq. (2)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # make more sense for attention modules than dropping individual attention\n        # scores.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer) # Eq. (3)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        return context_layer\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, hidden_size, hidden_dropout_prob, layer_norm_eps):\n        super().__init__()\n        self.dense = nn.Linear(hidden_size, hidden_size)\n        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor) # Residual connection + LayerNorm\n        return hidden_states\n\nclass BertAttention(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob, layer_norm_eps):\n        super().__init__()\n        self.self = BertSelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n        self.output = BertSelfOutput(hidden_size, hidden_dropout_prob, layer_norm_eps)\n\n    def forward(self, hidden_states, attention_mask=None):\n        self_output = self.self(hidden_states, attention_mask)\n        attention_output = self.output(self_output, hidden_states)\n        return attention_output\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, hidden_size, intermediate_size):\n        super().__init__()\n        self.dense = nn.Linear(hidden_size, intermediate_size)\n        # INFERRED: BERT uses GELU activation function, as per the original implementation.\n        self.intermediate_act_fn = nn.GELU()\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\nclass BertOutput(nn.Module):\n    def __init__(self, intermediate_size, hidden_size, hidden_dropout_prob, layer_norm_eps):\n        super().__init__()\n        self.dense = nn.Linear(intermediate_size, hidden_size)\n        self.LayerNorm = nn.LayerNorm(hidden_size, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor) # Residual connection + LayerNorm\n        return hidden_states\n\nclass BertLayer(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob, layer_norm_eps, intermediate_size):\n        super().__init__()\n        self.attention = BertAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob, layer_norm_eps)\n        self.intermediate = BertIntermediate(hidden_size, intermediate_size)\n        self.output = BertOutput(intermediate_size, hidden_size, hidden_dropout_prob, layer_norm_eps)\n\n    def forward(self, hidden_states, attention_mask=None):\n        attention_output = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\nclass BertEncoder(nn.Module):\n    def __init__(self, num_hidden_layers, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob, layer_norm_eps, intermediate_size):\n        super().__init__()\n        self.layer = nn.ModuleList([\n            BertLayer(\n                hidden_size,\n                num_attention_heads,\n                attention_probs_dropout_prob,\n                hidden_dropout_prob,\n                layer_norm_eps,\n                intermediate_size\n            )\n            for _ in range(num_hidden_layers)\n        ])\n\n    def forward(self, hidden_states, attention_mask=None):\n        for i, layer_module in enumerate(self.layer):\n            hidden_states = layer_module(hidden_states, attention_mask)\n        return hidden_states\n\nclass BertModel(nn.Module):\n    \"\"\"\n    The bare BERT Model transformer outputting raw hidden-states without any specific head on top.\n    \"\"\"\n    def __init__(self, vocab_size, hidden_size, num_attention_heads, num_hidden_layers,\n                 intermediate_size, max_position_embeddings, type_vocab_size,\n                 hidden_dropout_prob, attention_probs_dropout_prob, layer_norm_eps):\n        super().__init__()\n        self.embeddings = BertEmbeddings(\n            vocab_size, hidden_size, max_position_embeddings, type_vocab_size,\n            hidden_dropout_prob, layer_norm_eps\n        )\n        self.encoder = BertEncoder(\n            num_hidden_layers, hidden_size, num_attention_heads,\n            attention_probs_dropout_prob, hidden_dropout_prob, layer_norm_eps,\n            intermediate_size\n        )\n\n        # INFERRED: Pooler layer for sequence classification tasks, often used for [CLS] token output.\n        # This is part of the standard BERT architecture, though not strictly \"encoder\" output.\n        # It's a linear layer followed by tanh activation, as per the original BERT implementation.\n        self.pooler = nn.Linear(hidden_size, hidden_size)\n        self.pooler_activation = nn.Tanh()\n\n        self.init_weights()\n\n    def init_weights(self):\n        # INFERRED: Standard BERT weight initialization, typically a truncated normal distribution.\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=0.02)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None):\n        if input_ids is None:\n            # TODO: Handle case where input_ids is None, perhaps raise an error.\n            raise ValueError(\"input_ids must be provided for BertModel.\")\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # This attention mask is more simple than the triangular one used in causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast here.\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n\n        # Extended attention mask for broadcasting\n        # (batch_size, 1, 1, seq_length)\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw attention scores in the self-attention layer,\n        # the softmax will be nearly zero for the masked positions.\n        extended_attention_mask = extended_attention_mask.to(dtype=self.embeddings.word_embeddings.weight.dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids\n        )\n        encoder_output = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask\n        )\n\n        # Pooler output for [CLS] token\n        # INFERRED: The pooler takes the hidden state of the first token ([CLS])\n        # and applies a linear layer followed by a Tanh activation.\n        first_token_tensor = encoder_output[:, 0]\n        pooled_output = self.pooler(first_token_tensor)\n        pooled_output = self.pooler_activation(pooled_output)\n\n        return encoder_output, pooled_output\n\n# Component: Classification Head (Fine-tuning)\n# Provenance: inferred\n# Assumption: The architecture of the classification head (dropout layer followed by a linear layer) is inferred based on common practices for fine-tuning BERT-like models for sequence classification, as no specific architecture was detailed in the provided context.\n# Assumption: A default `dropout_prob` of 0.1 is assumed if not explicitly provided, consistent with typical BERT fine-tuning configurations.\n# Assumption: The `hidden_size` parameter, representing the BERT model's hidden state dimension, is a placeholder and must be provided from the specific BERT model configuration (e.g., 768 for BERT-base).\n# Assumption: The `num_labels` parameter, representing the number of classes for the downstream task, is a placeholder and must be provided based on the specific task requirements (e.g., 2 for binary classification).\nimport torch\nimport torch.nn as nn\n\nclass ClassificationHead(nn.Module):\n    \"\"\"\n    Classification Head for fine-tuning BERT on sequence classification tasks.\n    It takes the pooled output (usually the [CLS] token's representation)\n    and projects it to the number of output labels.\n    \"\"\"\n    def __init__(self, hidden_size: int, num_labels: int, dropout_prob: float = None):\n        super().__init__()\n        # INFERRED: A dropout layer is typically applied before the final classification layer\n        # for regularization during fine-tuning, as seen in official BERT implementations.\n        # ASSUMED: If dropout_prob is not provided, a common default of 0.1 is used,\n        # consistent with BERT's pre-training and fine-tuning practices.\n        self.dropout = nn.Dropout(dropout_prob if dropout_prob is not None else 0.1)\n        \n        # INFERRED: A linear layer is used to project the hidden state of the [CLS] token\n        # to the number of output classes for the specific classification task.\n        self.classifier = nn.Linear(hidden_size, num_labels)\n\n        # TODO: hidden_size - The dimension of the BERT model's hidden states.\n        # This value is typically 768 for BERT-base and 1024 for BERT-large.\n        # It must be provided from the BERT model configuration. # ASSUMED\n        \n        # TODO: num_labels - The number of classes for the specific downstream classification task.\n        # This value is task-dependent (e.g., 2 for binary classification, N for multi-class). # ASSUMED\n\n    def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the classification head.\n\n        Args:\n            pooled_output (torch.Tensor): The pooled output from the BERT model,\n                                          typically the final hidden state of the [CLS] token.\n                                          Shape: (batch_size, hidden_size)\n\n        Returns:\n            torch.Tensor: Logits for each class. Shape: (batch_size, num_labels)\n        \"\"\"\n        # Apply dropout for regularization\n        x = self.dropout(pooled_output)\n        \n        # Project the hidden state to the number of output labels\n        logits = self.classifier(x)\n        \n        return logits\n\n# Component: Final Hidden States (T_i)\n# Provenance: inferred\n# Assumption: Standard BERT architecture components are used as described in the paper.\n# Assumption: A01: The position embeddings are a learned lookup table of size (max_sequence_length, hidden_size), e.g., (512, 768). They are trained from scratch along with the rest of the model. For sequences longer than 512, a common strategy is to truncate the input, though this is not specified in the paper.\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math as Math # For Math.sqrt\n\n# ASSUMED: Standard BERT architecture components are used as described in the paper.\n# INFERRED: The final hidden states (T_i) are the output of the last layer of the Transformer encoder.\n\ndef gelu(x):\n    \"\"\"\n    Original Google BERT implementation uses this approximation of GELU.\n    \"\"\"\n    return x * 0.5 * (1.0 + torch.erf(x / Math.sqrt(2.0))) # Eq. (GELU approximation)\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"\n    Construct the embeddings from word, position and token_type embeddings.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.vocab_size = TODO_VOCAB_SIZE # INFERRED: From BERT pre-training, e.g., 30522 for uncased base.\n        self.hidden_size = TODO_HIDDEN_SIZE # INFERRED: From BERT base config, e.g., 768.\n        self.max_position_embeddings = TODO_MAX_POSITION_EMBEDDINGS # ASSUMED: A01, e.g., 512.\n        self.type_vocab_size = TODO_TYPE_VOCAB_SIZE # INFERRED: From BERT config, usually 2 (segment A, segment B).\n        self.hidden_dropout_prob = TODO_HIDDEN_DROPOUT_PROB # INFERRED: From BERT config, e.g., 0.1.\n        self.layer_norm_eps = TODO_LAYER_NORM_EPS # INFERRED: From BERT config, e.g., 1e-12.\n\n        self.word_embeddings = nn.Embedding(self.vocab_size, self.hidden_size)\n        self.position_embeddings = nn.Embedding(self.max_position_embeddings, self.hidden_size) # ASSUMED: A01\n        self.token_type_embeddings = nn.Embedding(self.type_vocab_size, self.hidden_size)\n\n        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps) # Eq. (Layer Normalization)\n        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n\n    def forward(self, input_ids=None, token_type_ids=None, position_ids=None):\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings # Eq. (Embedding Summation)\n        embeddings = self.LayerNorm(embeddings) # Eq. (Layer Normalization)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden_size = TODO_HIDDEN_SIZE # INFERRED: From BERT base config, e.g., 768.\n        self.num_attention_heads = TODO_NUM_ATTENTION_HEADS # INFERRED: From BERT base config, e.g., 12.\n        self.attention_probs_dropout_prob = TODO_ATTENTION_PROBS_DROPOUT_PROB # INFERRED: From BERT config, e.g., 0.1.\n        self.attention_head_size = self.hidden_size // self.num_attention_heads # INFERRED: Standard calculation.\n        self.all_head_size = self.num_attention_heads * self.attention_head_size # INFERRED: Standard calculation.\n\n        self.query = nn.Linear(self.hidden_size, self.all_head_size)\n        self.key = nn.Linear(self.hidden_size, self.all_head_size)\n        self.value = nn.Linear(self.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(self.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3) # (batch_size, num_heads, seq_len, head_dim)\n\n    def forward(self, hidden_states, attention_mask=None):\n        query_layer = self.query(hidden_states)\n        key_layer = self.key(hidden_states)\n        value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(query_layer)\n        key_layer = self.transpose_for_scores(key_layer)\n        value_layer = self.transpose_for_scores(value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # Eq. (Scaled Dot-Product Attention - part 1)\n        attention_scores = attention_scores / Math.sqrt(self.attention_head_size) # Eq. (Scaled Dot-Product Attention - part 2)\n\n        if attention_mask is not None:\n            # Apply the attention mask (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask # Eq. (Masking for attention scores)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores) # Eq. (Softmax)\n\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer) # Eq. (Scaled Dot-Product Attention - part 3)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        return context_layer\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden_size = TODO_HIDDEN_SIZE # INFERRED: From BERT base config, e.g., 768.\n        self.hidden_dropout_prob = TODO_HIDDEN_DROPOUT_PROB # INFERRED: From BERT config, e.g., 0.1.\n        self.layer_norm_eps = TODO_LAYER_NORM_EPS # INFERRED: From BERT config, e.g., 1e-12.\n\n        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps) # Eq. (Layer Normalization)\n        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor) # Eq. (Residual Connection + Layer Normalization)\n        return hidden_states\n\nclass BertAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.self = BertSelfAttention()\n        self.output = BertSelfOutput()\n\n    def forward(self, hidden_states, attention_mask=None):\n        self_output = self.self(hidden_states, attention_mask)\n        attention_output = self.output(self_output, hidden_states)\n        return attention_output\n\nclass BertIntermediate(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden_size = TODO_HIDDEN_SIZE # INFERRED: From BERT base config, e.g., 768.\n        self.intermediate_size = TODO_INTERMEDIATE_SIZE # INFERRED: From BERT base config, e.g., 3072.\n        self.hidden_act = TODO_HIDDEN_ACT # INFERRED: From BERT config, e.g., 'gelu'.\n\n        self.dense = nn.Linear(self.hidden_size, self.intermediate_size)\n        if self.hidden_act == \"gelu\":\n            self.intermediate_act_fn = gelu\n        elif self.hidden_act == \"relu\":\n            self.intermediate_act_fn = F.relu\n        else:\n            raise ValueError(f\"Unsupported activation function: {self.hidden_act}\")\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states) # Eq. (Activation Function)\n        return hidden_states\n\nclass BertOutput(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden_size = TODO_HIDDEN_SIZE # INFERRED: From BERT base config, e.g., 768.\n        self.intermediate_size = TODO_INTERMEDIATE_SIZE # INFERRED: From BERT base config, e.g., 3072.\n        self.hidden_dropout_prob = TODO_HIDDEN_DROPOUT_PROB # INFERRED: From BERT config, e.g., 0.1.\n        self.layer_norm_eps = TODO_LAYER_NORM_EPS # INFERRED: From BERT config, e.g., 1e-12.\n\n        self.dense = nn.Linear(self.intermediate_size, self.hidden_size)\n        self.LayerNorm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps) # Eq. (Layer Normalization)\n        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor) # Eq. (Residual Connection + Layer Normalization)\n        return hidden_states\n\nclass BertLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.attention = BertAttention()\n        self.intermediate = BertIntermediate()\n        self.output = BertOutput()\n\n    def forward(self, hidden_states, attention_mask=None):\n        attention_output = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\nclass BertEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.num_hidden_layers = TODO_NUM_HIDDEN_LAYERS # INFERRED: From BERT base config, e.g., 12.\n        self.layer = nn.ModuleList([BertLayer() for _ in range(self.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask=None):\n        # The paper states \"The final hidden state T_i for each input token i\"\n        # This implies the output of the last layer.\n        for i, layer_module in enumerate(self.layer):\n            hidden_states = layer_module(hidden_states, attention_mask)\n        return hidden_states # This is T_i\n\nclass BertModel(nn.Module):\n    \"\"\"\n    The bare BERT Model transformer outputting raw hidden-states (T_i) without any specific head on top.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.embeddings = BertEmbeddings()\n        self.encoder = BertEncoder()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are (batch_size, 1, 1, to_seq_length)\n        # So we can broadcast to (batch_size, num_heads, from_seq_length, to_seq_length)\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # This effectively masks out attention to padded tokens by setting their scores to a very small number.\n        extended_attention_mask = extended_attention_mask.to(dtype=self.embeddings.word_embeddings.weight.dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 # Eq. (Masking for attention scores)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask\n        )\n        # The final hidden states (T_i) are the output of the BertEncoder\n        final_hidden_states = encoder_outputs\n        return final_hidden_states\n```"
      },
      {
        "id": "ambiguity",
        "title": "5. Missing Details",
        "content": "## A01: Position Embedding Implementation\n- Type: underspecified_architecture\n- Section: 3 BERT, Input/Output Representations\n- Ambiguous point: The paper states that position embeddings are learned, not the fixed sinusoidal embeddings from the original Transformer paper. However, it does not specify how they are learned, their maximum length, or how the model would handle sequences longer than the 512 tokens seen during pre-training.\n- Implementation consequence: If an implementer assumes a fixed maximum length (e.g., 512) for the learned position embeddings, the model will fail on longer sequences at inference time. If the embeddings are not initialized or trained correctly, it could degrade model performance, as positional information is critical for the self-attention mechanism.\n- Agent resolution: Assume the position embeddings are a learned lookup table of size (max_sequence_length, hidden_size), e.g., (512, 768). They are trained from scratch along with the rest of the model. For sequences longer than 512, a common strategy is to truncate the input, though this is not specified in the paper.\n- Confidence: 0.5\n\n## A02: Tokenizer Vocabulary Generation\n- Type: missing_training_detail\n- Section: 3 BERT, Input/Output Representations\n- Ambiguous point: The paper specifies a 30,000 token WordPiece vocabulary but does not detail its creation process. It's unclear what corpus was used to train the tokenizer, whether it was cased or uncased for pre-training, or what other configuration settings were used.\n- Implementation consequence: Using a different vocabulary or tokenizer settings would create a complete mismatch with the released pre-trained weights, making it impossible to replicate the paper's results. The model's performance is highly sensitive to the tokenization scheme.\n- Agent resolution: The official BERT implementation released by Google uses a specific vocabulary file (`vocab.txt`). It is standard practice to use this provided file. The base model is uncased, and a separate cased model was also released. The pre-training described for the main results likely used the uncased vocabulary.\n- Confidence: 0.5\n\n## A03: NSP Random Sentence Sampling Strategy\n- Type: missing_training_detail\n- Section: 3.1 Pre-training BERT, Task #2: Next Sentence Prediction (NSP)\n- Ambiguous point: For the Next Sentence Prediction task, the paper states that 50% of the time, sentence B is a 'random sentence from the corpus'. It does not specify the sampling strategy: is the random sentence from the same document, a different document, or the entire corpus? Are there constraints on its length?\n- Implementation consequence: The difficulty of the NSP task depends heavily on this sampling strategy. If random sentences are always from different documents, the model might learn to solve the task using simple topic differences, rather than learning about coherence and logical flow. This could make the pre-training less effective for downstream NLI tasks.\n- Agent resolution: The official implementation samples random sentences from the entire corpus, not just the same document. A sentence is chosen at random, and there are no explicit constraints other than it not being the true next sentence.\n- Confidence: 0.5\n\n## A04: SQuAD v2.0 No-Answer Threshold (τ)\n- Type: missing_hyperparameter\n- Section: 4.3 SQuAD v2.0\n- Ambiguous point: For SQuAD v2.0, the model predicts a no-answer response if the score of the best non-null span is not greater than the no-answer span score by a threshold τ. The paper states τ is 'selected on the dev set to maximize F1' but does not provide the value of τ or the search procedure.\n- Implementation consequence: Without the value of τ, the exact F1 score on the SQuAD v2.0 dev and test sets cannot be replicated. Different values of τ will produce a different precision/recall trade-off for answerable vs. unanswerable questions, leading to different results.\n- Agent resolution: The value of τ must be found by running inference on the development set with the fine-tuned model and searching for the threshold that maximizes the F1 score. A common approach is to iterate through a range of possible score differences observed on the dev set and pick the one that yields the best F1.\n- Confidence: 0.5\n\n## A05: TriviaQA Augmentation Details for SQuAD\n- Type: missing_training_detail\n- Section: 4.2 SQuAD v1.1, Footnote 12\n- Ambiguous point: The best SQuAD v1.1 model was first fine-tuned on TriviaQA. The paper provides minimal details on this intermediate step: 'first 400 tokens in documents, that contain at least one of the provided possible answers'. Key details like the learning rate, number of epochs, and batch size for this phase are missing.\n- Implementation consequence: The state-of-the-art SQuAD v1.1 results are not reproducible without these crucial hyperparameters. The performance boost from this intermediate training step is significant, and incorrect settings could lead to worse results or negative transfer.\n- Agent resolution: Assume the same fine-tuning hyperparameters as the main SQuAD task (e.g., LR=5e-5, Batch=32) and train for a similar number of epochs (e.g., 2-3). This is a reasonable starting point for experimentation.\n- Confidence: 0.5\n\n## A06: Details of Random Restarts\n- Type: missing_training_detail\n- Section: 4.1 GLUE\n- Ambiguous point: For BERT_LARGE on GLUE, the authors 'ran several random restarts and selected the best model on the Dev set'. The paper does not specify how many restarts 'several' is, nor what was re-randomized (data shuffling, classifier layer initialization, or both).\n- Implementation consequence: The reported GLUE scores for BERT_LARGE might be the result of cherry-picking from an unknown number of runs. This makes it difficult to assess the model's expected performance and variance. An implementer might get a lower score with a single run and incorrectly assume their implementation is flawed.\n- Agent resolution: Implementers should be aware that fine-tuning can be unstable. A common practice is to run experiments with 3 to 5 different random seeds and report the mean and standard deviation. To replicate the paper's 'best' score, one would need to run multiple trials and select the best-performing one on the dev set.\n- Confidence: 0.5\n\n## A07: Pre-training Loss Weighting\n- Type: ambiguous_loss_function\n- Section: A.2 Pre-training Procedure\n- Ambiguous point: The total pre-training loss is described as 'the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.' It is not explicitly stated if these two loss components are weighted equally (i.e., weight of 1.0 for each) or if there is some other weighting scheme.\n- Implementation consequence: If the two losses are not weighted equally, the model's focus during pre-training would shift. For example, a higher weight on NSP might make the model better at sentence-pair tasks at the expense of token-level understanding. An incorrect implementation of the loss function would lead to a differently optimized pre-trained model.\n- Agent resolution: Assume the losses are added with equal weight (1.0). The loss function is `Loss = Loss_MLM + Loss_NSP`.\n- Confidence: 0.5"
      },
      {
        "id": "training",
        "title": "6. Training Recipe",
        "content": "### Hyperparameter Registry\n\n| name | value | source | status | suggested_default |\n| --- | --- | --- | --- | --- |\n| BERT_BASE: Number of Layers (L) | 12 | 3 BERT, Model Architecture | paper-stated |  |\n| BERT_BASE: Hidden Size (H) | 768 | 3 BERT, Model Architecture | paper-stated |  |\n| BERT_BASE: Number of Attention Heads (A) | 12 | 3 BERT, Model Architecture | paper-stated |  |\n| BERT_BASE: Total Parameters | 110M | 3 BERT, Model Architecture | paper-stated |  |\n| BERT_BASE: Feed-forward/Filter Size | 3072 | 3 BERT, Model Architecture, Footnote 3 | paper-stated |  |\n| BERT_LARGE: Number of Layers (L) | 24 | 3 BERT, Model Architecture | paper-stated |  |\n| BERT_LARGE: Hidden Size (H) | 1024 | 3 BERT, Model Architecture | paper-stated |  |\n| BERT_LARGE: Number of Attention Heads (A) | 16 | 3 BERT, Model Architecture | paper-stated |  |\n| BERT_LARGE: Total Parameters | 340M | 3 BERT, Model Architecture | paper-stated |  |\n| BERT_LARGE: Feed-forward/Filter Size | 4096 | 3 BERT, Model Architecture, Footnote 3 | paper-stated |  |\n| Vocabulary Size | 30000 | 3 BERT, Input/Output Representations | paper-stated |  |\n| Tokenizer | WordPiece | 3 BERT, Input/Output Representations | paper-stated |  |\n| Pre-training: Masking Percentage (MLM) | 0.15 | 3.1 Pre-training BERT, Task #1: Masked LM | paper-stated |  |\n| Pre-training: Masking Strategy - [MASK] token | 0.8 | 3.1 Pre-training BERT, Task #1: Masked LM | paper-stated |  |\n| Pre-training: Masking Strategy - Random token | 0.1 | 3.1 Pre-training BERT, Task #1: Masked LM | paper-stated |  |\n| Pre-training: Masking Strategy - Unchanged token | 0.1 | 3.1 Pre-training BERT, Task #1: Masked LM | paper-stated |  |\n| Pre-training: Next Sentence Label Ratio (IsNext/NotNext) | 0.5 | 3.1 Pre-training BERT, Task #2: Next Sentence Prediction (NSP) | paper-stated |  |\n| Pre-training: Max Sequence Length | 512 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Batch Size | 256 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Total Steps | 1000000 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Number of Epochs | 40 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Optimizer | Adam | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Learning Rate | 1e-4 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Adam beta_1 | 0.9 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Adam beta_2 | 0.999 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: L2 Weight Decay | 0.01 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Learning Rate Warmup Steps | 10000 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Learning Rate Decay Schedule | linear | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Dropout Probability | 0.1 | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Activation Function | GELU | A.2 Pre-training Procedure | paper-stated |  |\n| Pre-training: Sequence Length Schedule | 90% steps at length 128, 10% steps at length 512 | A.2 Pre-training Procedure | paper-stated |  |\n| Fine-tuning: Dropout Probability | 0.1 | A.3 Fine-tuning Procedure | paper-stated |  |\n| Fine-tuning: Batch Size (Search Range) | [16, 32] | A.3 Fine-tuning Procedure | paper-stated |  |\n| Fine-tuning: Learning Rate (Search Range) | [5e-5, 3e-5, 2e-5] | A.3 Fine-tuning Procedure | paper-stated |  |\n| Fine-tuning: Number of Epochs (Search Range) | [2, 3, 4] | A.3 Fine-tuning Procedure | paper-stated |  |\n| Fine-tuning (GLUE): Batch Size | 32 | 4.1 GLUE | paper-stated |  |\n| Fine-tuning (GLUE): Number of Epochs | 3 | 4.1 GLUE | paper-stated |  |\n| Fine-tuning (GLUE): Learning Rate (Selected from) | [5e-5, 4e-5, 3e-5, 2e-5] | 4.1 GLUE | paper-stated |  |\n| Fine-tuning (SQuAD v1.1): Number of Epochs | 3 | 4.2 SQuAD v1.1 | paper-stated |  |\n| Fine-tuning (SQuAD v1.1): Learning Rate | 5e-5 | 4.2 SQuAD v1.1 | paper-stated |  |\n| Fine-tuning (SQuAD v1.1): Batch Size | 32 | 4.2 SQuAD v1.1 | paper-stated |  |\n| Fine-tuning (SQuAD v2.0): Number of Epochs | 2 | 4.3 SQuAD v2.0 | paper-stated |  |\n| Fine-tuning (SQuAD v2.0): Learning Rate | 5e-5 | 4.3 SQuAD v2.0 | paper-stated |  |\n| Fine-tuning (SQuAD v2.0): Batch Size | 48 | 4.3 SQuAD v2.0 | paper-stated |  |\n| Fine-tuning (SWAG): Number of Epochs | 3 | 4.4 SWAG | paper-stated |  |\n| Fine-tuning (SWAG): Learning Rate | 2e-5 | 4.4 SWAG | paper-stated |  |\n| Fine-tuning (SWAG): Batch Size | 16 | 4.4 SWAG | paper-stated |  |\n| Feature-based NER: BiLSTM Layers | 2 | 5.3 Feature-based Approach with BERT | paper-stated |  |\n| Feature-based NER: BiLSTM Hidden Size | 768 | 5.3 Feature-based Approach with BERT | paper-stated |  |"
      }
    ],
    "downloads": [
      {
        "label": "architecture_summary.md",
        "href": "/demo-artifacts/bert/architecture_summary.md"
      },
      {
        "label": "annotated_code.py",
        "href": "/demo-artifacts/bert/annotated_code.py"
      },
      {
        "label": "hyperparameters.csv",
        "href": "/demo-artifacts/bert/hyperparameters.csv"
      },
      {
        "label": "ambiguity_report.md",
        "href": "/demo-artifacts/bert/ambiguity_report.md"
      }
    ]
  }
};

export const EXAMPLE_LIST: ExampleWalkthrough[] = Object.values(EXAMPLE_WALKTHROUGHS);
