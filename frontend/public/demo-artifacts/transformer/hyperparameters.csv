name,value,source,status,suggested_default
N (Encoder/Decoder Layers),6,3.1 Encoder and Decoder Stacks,paper-stated,
d_model (Model Dimension),512,3.1 Encoder and Decoder Stacks,paper-stated,
h (Number of Attention Heads),8,3.2.2 Multi-Head Attention,paper-stated,
d_k (Key Dimension),64,3.2.2 Multi-Head Attention,paper-stated,
d_v (Value Dimension),64,3.2.2 Multi-Head Attention,paper-stated,
d_ff (Feed-Forward Inner Dimension),2048,3.3 Position-wise Feed-Forward Networks,paper-stated,
"P_drop (Dropout Rate, base model)",0.1,5.4 Regularization,paper-stated,
epsilon_ls (Label Smoothing),0.1,5.4 Regularization,paper-stated,
Optimizer,Adam,5.3 Optimizer,paper-stated,
Adam beta1,0.9,5.3 Optimizer,paper-stated,
Adam beta2,0.98,5.3 Optimizer,paper-stated,
Adam epsilon,1e-9,5.3 Optimizer,paper-stated,
warmup_steps (Learning Rate Schedule),4000,5.3 Optimizer,paper-stated,
EN-DE Vocabulary Size,37000,5.1 Training Data and Batching,paper-stated,
EN-FR Vocabulary Size,32000,5.1 Training Data and Batching,paper-stated,
Batch Size (Tokens per batch),25000 source tokens and 25000 target tokens,5.1 Training Data and Batching,paper-stated,
Training Steps (base model),100000,5.2 Hardware and Schedule,paper-stated,
Training Steps (big model),300000,5.2 Hardware and Schedule,paper-stated,
Hardware,8 NVIDIA P100 GPUs,5.2 Hardware and Schedule,paper-stated,
Beam Size (Inference),4,6.1 Machine Translation,paper-stated,
Length Penalty alpha (Inference),0.6,6.1 Machine Translation,paper-stated,
Positional Encoding Wavelength Max,10000,3.5 Positional Encoding,paper-stated,
N (big model),6,Table 3: Variations on the Transformer architecture,paper-stated,
d_model (big model),1024,Table 3: Variations on the Transformer architecture,paper-stated,
d_ff (big model),4096,Table 3: Variations on the Transformer architecture,paper-stated,
h (big model),16,Table 3: Variations on the Transformer architecture,paper-stated,
d_k (big model),inferred,Table 3: Variations on the Transformer architecture,inferred,64
d_v (big model),inferred,Table 3: Variations on the Transformer architecture,inferred,64
"P_drop (big model, EN-DE)",0.3,Table 3: Variations on the Transformer architecture,paper-stated,
"P_drop (big model, EN-FR)",0.1,6.1 Machine Translation,paper-stated,
N (Constituency Parsing),4,6.3 English Constituency Parsing,paper-stated,
d_model (Constituency Parsing),1024,6.3 English Constituency Parsing,paper-stated,
WSJ-only Vocabulary Size (Parsing),16000,6.3 English Constituency Parsing,paper-stated,
Semi-supervised Vocabulary Size (Parsing),32000,6.3 English Constituency Parsing,paper-stated,
Beam Size (Parsing),21,6.3 English Constituency Parsing,paper-stated,
Length Penalty alpha (Parsing),0.3,6.3 English Constituency Parsing,paper-stated,
Max Output Length (Inference),input length + 50,6.1 Machine Translation,paper-stated,
Max Output Length (Parsing),input length + 300,6.3 English Constituency Parsing,paper-stated,
