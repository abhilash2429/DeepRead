name,value,source,status,suggested_default
BERT_BASE: Number of Layers (L),12,"3 BERT, Model Architecture",paper-stated,
BERT_BASE: Hidden Size (H),768,"3 BERT, Model Architecture",paper-stated,
BERT_BASE: Number of Attention Heads (A),12,"3 BERT, Model Architecture",paper-stated,
BERT_BASE: Total Parameters,110M,"3 BERT, Model Architecture",paper-stated,
BERT_BASE: Feed-forward/Filter Size,3072,"3 BERT, Model Architecture, Footnote 3",paper-stated,
BERT_LARGE: Number of Layers (L),24,"3 BERT, Model Architecture",paper-stated,
BERT_LARGE: Hidden Size (H),1024,"3 BERT, Model Architecture",paper-stated,
BERT_LARGE: Number of Attention Heads (A),16,"3 BERT, Model Architecture",paper-stated,
BERT_LARGE: Total Parameters,340M,"3 BERT, Model Architecture",paper-stated,
BERT_LARGE: Feed-forward/Filter Size,4096,"3 BERT, Model Architecture, Footnote 3",paper-stated,
Vocabulary Size,30000,"3 BERT, Input/Output Representations",paper-stated,
Tokenizer,WordPiece,"3 BERT, Input/Output Representations",paper-stated,
Pre-training: Masking Percentage (MLM),0.15,"3.1 Pre-training BERT, Task #1: Masked LM",paper-stated,
Pre-training: Masking Strategy - [MASK] token,0.8,"3.1 Pre-training BERT, Task #1: Masked LM",paper-stated,
Pre-training: Masking Strategy - Random token,0.1,"3.1 Pre-training BERT, Task #1: Masked LM",paper-stated,
Pre-training: Masking Strategy - Unchanged token,0.1,"3.1 Pre-training BERT, Task #1: Masked LM",paper-stated,
Pre-training: Next Sentence Label Ratio (IsNext/NotNext),0.5,"3.1 Pre-training BERT, Task #2: Next Sentence Prediction (NSP)",paper-stated,
Pre-training: Max Sequence Length,512,A.2 Pre-training Procedure,paper-stated,
Pre-training: Batch Size,256,A.2 Pre-training Procedure,paper-stated,
Pre-training: Total Steps,1000000,A.2 Pre-training Procedure,paper-stated,
Pre-training: Number of Epochs,40,A.2 Pre-training Procedure,paper-stated,
Pre-training: Optimizer,Adam,A.2 Pre-training Procedure,paper-stated,
Pre-training: Learning Rate,1e-4,A.2 Pre-training Procedure,paper-stated,
Pre-training: Adam beta_1,0.9,A.2 Pre-training Procedure,paper-stated,
Pre-training: Adam beta_2,0.999,A.2 Pre-training Procedure,paper-stated,
Pre-training: L2 Weight Decay,0.01,A.2 Pre-training Procedure,paper-stated,
Pre-training: Learning Rate Warmup Steps,10000,A.2 Pre-training Procedure,paper-stated,
Pre-training: Learning Rate Decay Schedule,linear,A.2 Pre-training Procedure,paper-stated,
Pre-training: Dropout Probability,0.1,A.2 Pre-training Procedure,paper-stated,
Pre-training: Activation Function,GELU,A.2 Pre-training Procedure,paper-stated,
Pre-training: Sequence Length Schedule,"90% steps at length 128, 10% steps at length 512",A.2 Pre-training Procedure,paper-stated,
Fine-tuning: Dropout Probability,0.1,A.3 Fine-tuning Procedure,paper-stated,
Fine-tuning: Batch Size (Search Range),"[16, 32]",A.3 Fine-tuning Procedure,paper-stated,
Fine-tuning: Learning Rate (Search Range),"[5e-5, 3e-5, 2e-5]",A.3 Fine-tuning Procedure,paper-stated,
Fine-tuning: Number of Epochs (Search Range),"[2, 3, 4]",A.3 Fine-tuning Procedure,paper-stated,
Fine-tuning (GLUE): Batch Size,32,4.1 GLUE,paper-stated,
Fine-tuning (GLUE): Number of Epochs,3,4.1 GLUE,paper-stated,
Fine-tuning (GLUE): Learning Rate (Selected from),"[5e-5, 4e-5, 3e-5, 2e-5]",4.1 GLUE,paper-stated,
Fine-tuning (SQuAD v1.1): Number of Epochs,3,4.2 SQuAD v1.1,paper-stated,
Fine-tuning (SQuAD v1.1): Learning Rate,5e-5,4.2 SQuAD v1.1,paper-stated,
Fine-tuning (SQuAD v1.1): Batch Size,32,4.2 SQuAD v1.1,paper-stated,
Fine-tuning (SQuAD v2.0): Number of Epochs,2,4.3 SQuAD v2.0,paper-stated,
Fine-tuning (SQuAD v2.0): Learning Rate,5e-5,4.3 SQuAD v2.0,paper-stated,
Fine-tuning (SQuAD v2.0): Batch Size,48,4.3 SQuAD v2.0,paper-stated,
Fine-tuning (SWAG): Number of Epochs,3,4.4 SWAG,paper-stated,
Fine-tuning (SWAG): Learning Rate,2e-5,4.4 SWAG,paper-stated,
Fine-tuning (SWAG): Batch Size,16,4.4 SWAG,paper-stated,
Feature-based NER: BiLSTM Layers,2,5.3 Feature-based Approach with BERT,paper-stated,
Feature-based NER: BiLSTM Hidden Size,768,5.3 Feature-based Approach with BERT,paper-stated,
